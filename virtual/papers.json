[{"content":{"TLDR":"","abstract":"Knowledge graph (KG) representation learning techniques that learn continuous embeddings of entities and relations in the KG have become popular in many AI applications. With a large KG, the embeddings consume a large amount of storage and memory. This is problematic and prohibits the deployment of these techniques in many real world settings. Thus, we propose an approach that compresses the KG embedding layer by representing each entity in the KG as a vector of discrete codes and then composes the embeddings from these codes. The approach can be trained end-to-end with simple modifications to any existing KG embedding technique. We evaluate the approach on various standard KG embedding evaluations and show that it achieves 50-1000x compression of embeddings with a minor loss in performance. The compressed embeddings also retain the ability to perform various reasoning tasks such as KG inference.","authors":["Mrinmaya Sachan"],"keywords":[],"pdf_url":"","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"/session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"Knowledge Graph Embedding Compression","youtube":"kBmU2pPMHCA"},"forum":"SOEJGCE76x","id":"76"},{"content":{"TLDR":"","abstract":"Given a set of common concepts like {apple (noun), pick (verb), tree (noun)},  humans find it easy to write a sentence describing a grammatical and logically coherent scenario that covers these concepts,  for example, {a boy picks an apple from a tree''}.  The process of generating these sentences requires humans to use commonsense knowledge. We denote this ability as generative commonsense reasoning. Recent work in commonsense reasoning has focused mainly on discriminating the most plausible scenes from distractors via natural language understanding (NLU) settings such as multi-choice question answering. However, generative commonsense reasoning is a relatively unexplored research area, primarily due to the lack of a specialized benchmark dataset.\n\nIn this paper, we present a constrained natural language generation (NLG) dataset, named CommonGen, to explicitly challenge machines in generative commonsense reasoning. It consists of 30k concept-sets with human-written sentences as references. Crowd-workers were also asked to write the rationales (i.e. the commonsense facts) used for generating the sentences in the development and test sets. We conduct experiments on a variety of generation models with both automatic and human evaluation. Experimental results show that there is still a large gap between the current state-of-the-art pre-trained model, UniLM, and human performance.","authors":["Bill Yuchen Lin","Ming Shen","Wangchunshu Zhou","Pei Zhou","Chandra Bhagavatula","Yejin Choi","Xiang Ren"],"keywords":[],"pdf_url":"","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"/session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning","youtube":"WwjyXDuoPtk"},"forum":"yuD2q50HWv","id":"75"},{"content":{"TLDR":"We propose a reliable way to generate citation field extraction dataset from BibTeX. Training models on our dataset achieves new SoTa on UMass CFE dataset.","abstract":"Accurate parsing of citation reference strings is crucial to automatically construct scholarly databases such as Google Scholar or Semantic Scholar. Citation field extraction (CFE) is precisely this task---given a reference label which tokens refer to the authors, venue, title,  editor, journal, pages, etc. Most methods for CFE are supervised and rely on training from labeled datasets that are quite small compared to the great variety of reference formats. BibTeX, the widely used reference management tool, provides a natural method to automatically generate and label training data for CFE. In this paper, we describe a technique for using BibTeX to generate, automatically, a large-scale 41M labeled strings), labeled dataset, that is four orders of magnitude larger than the current largest CFE dataset, namely the UMass Citation Field Extraction dataset [Anzaroot and McCallum, 2013]. We experimentally demonstrate how our dataset can be used to improve the performance of the UMass CFE using a RoBERTa-based [Liu et al., 2019] model. In comparison to previous SoTA, we achieve a 24.48% relative error reduction, achieving span level F1-scores of 96.3%.","authors":["Dung Thai","Zhiyang Xu","Nicholas Monath","Boris Veytsman","Andrew McCallum"],"keywords":["sequence labeling","information extraction","auto-generated dataset"],"pdf_url":"/pdf/c2006bb2e33b09f6f4e0fa73c0c728119982dec0.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"/session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Using BibTeX to Automatically Generate Labeled Data for Citation Field Extraction","youtube":"J-vpIctDLms"},"forum":"OnUd3hf3o3","id":"85"},{"content":{"TLDR":"We achieve state of the art on CoNLL and TAC-KBP 2010 with a four layer transformer","abstract":"In this work, we present an entity linking model which combines a Transformer architecture with large scale pretraining from Wikipedia links.  Our model achieves the state-of-the-art on two commonly used entity linking datasets:  96.7% on CoNLL and 94.9% on TAC-KBP. We present detailed analyses to understand what design choices are important for entity linking, including choices of negative entity candidates, Transformer architecture, and input perturbations.  Lastly, we present promising results on more challenging settings such as end-to-end entity linking and entity linking without in-domain training data","authors":["Thibault F\u00e9vry","Nicholas FitzGerald","Livio Baldini Soares","Tom Kwiatkowski"],"keywords":["Entity linking","Pre-training","Wikification"],"pdf_url":"/pdf/98236ce89e5e6d8f6cfd7e77a550638d7470d7d6.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"/session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Empirical Evaluation of Pretraining Strategies for Supervised Entity Linking","youtube":"qteiBwBdR24"},"forum":"iHXV8UGYyL","id":"83"},{"content":{"TLDR":"We study entity linking for Chinese news comment and propose a novel attention based method to detect relevant context and supporting entities from reference articles.","abstract":"Automatic identification of mentioned entities in social media posts facilitates quick digestion of trending topics and popular opinions. Nonetheless, this remains a challenging task due to limited context and diverse name variations. In this paper, we study the problem of entity linking for Chinese news comments given mentions' spans. We hypothesize that comments often refer to entities in the corresponding news article, as well as topics involving the entities. We therefore propose a novel model, XREF, that leverages attention mechanisms to (1) pinpoint relevant context within comments, and (2) detect supporting entities from the news article. To improve training, we make two contributions: (a) we propose a supervised attention loss in addition to the standard cross entropy, and (b) we develop a weakly supervised training scheme to utilize the large-scale unlabeled corpus. Two new datasets in entertainment and product domains are collected and annotated for experiments. Our proposed method outperforms previous methods on both datasets. ","authors":["Xinyu Hua","Lei Li","Lifeng Hua","Lu Wang"],"keywords":["Entity Linking","Chinese social media","Data Augmentation"],"pdf_url":"/pdf/d3f077646fd700da8a67891c7dcb9f9eca9e9c44.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"/session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"XREF: Entity Linking for Chinese News Comments with Supplementary Article Reference","youtube":"XJCLlr3rdT0"},"forum":"1hLH6CKIjN","id":"42"},{"content":{"TLDR":"","abstract":"When pre-trained on large unsupervised textual corpora, language models are able to store and retrieve factual knowledge to some extent, making it possible to use them directly for zero-shot cloze-style question answering. However, storing factual knowledge in a fixed number of weights of a language model clearly has limitations. Previous approaches have successfully provided access to information outside the model weights using supervised architectures that combine an information retrieval system with a machine reading component. In this paper, we go one step further and integrate information from a retrieval system with a pre-trained language model in a purely unsupervised way. We report that augmenting pre-trained language models in this way dramatically improves performance and that it is competitive with a supervised machine reading baseline without requiring any supervised training. Furthermore, processing query and context with different segment tokens allows BERT to utilize its Next Sentence Prediction pre-trained classifier to determine whether the context is relevant or not, substantially improving BERT's zero-shot cloze-style question-answering performance and making its predictions robust to noisy contexts.","authors":["Fabio Petroni","Patrick Lewis","Aleksandra Piktus","Tim Rockt\u00e4schel","Yuxiang Wu","Alexander H. Miller","Sebastian Riedel"],"keywords":[],"pdf_url":"/pdf/9bc20e38dd19b074f9d563a2d1bf5ecbc90d6294.pdf","session":[{"calendarId":"poster","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"11:20","link":"/session_d1posters.html","sessionId":"d1posters","start":"10:20","title":"Day 1 Poster Session"}],"title":"How Context Affects Language Models' Factual Predictions","youtube":"2BURBZF5GmY"},"forum":"025X0zPfn","id":"21"},{"content":{"TLDR":"learning tractable models for imprecise probabilities ","abstract":"Probabilistic representations, such as Bayesian and Markov networks, are fundamental to much of statistical machine learning. Thus, learning probabilistic representations directly from data is a deep challenge, the main computational bottleneck being inference that is intractable. Tractable learning is a powerful new paradigm that attempts to learn distributions that support efficient probabilistic querying. By leveraging local structure, representations such as sum-product networks (SPNs) can capture high tree-width models with many hidden layers, essentially a deep architecture, while still admitting a range of probabilistic queries to be computable in time polynomial in the network size.  While the progress is impressive, numerous data sources are incomplete, and in the presence of missing data, structure learning methods nonetheless revert to  single distributions without  characterizing the loss in confidence. In recent work, credal sum-product networks, an imprecise extension of sum-product networks, were proposed to capture this robustness angle. In this work, we are interested in how such representations can be learnt and thus study how the computational machinery underlying tractable learning and inference can be generalized for imprecise probabilities. \n","authors":["Amelie Levray","Vaishak Belle"],"keywords":["credal networks","imprecise probabilities","tractable learning"],"pdf_url":"/pdf/961305c5ab559cf0e3017db830b746377472625b.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"/session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"Learning Credal Sum-Product Networks","youtube":"IJtLSzsQYaQ"},"forum":"3-Tc21z1Ub","id":"12"},{"content":{"TLDR":"A new paradigm for deep contextualized knowledge graph embeddings","abstract":"We introduce Dolores, a new knowledge graph embeddings, that effectively capture contextual cues and dependencies among entities and relations. First, we note that short paths on knowledge graphs comprising of chains of entities and relations can encode valuable information regarding their contextual usage. We operationalize this notion by representing knowledge graphs not as a collection of triples but as a collection of entity-relation chains, and learn embeddings using deep neural models that capture such contextual usage. Based on BiLSTMs, our model learns deep representations from constructed entity-relation chains. We show that these representations can be easily incorporated into existing models to significantly advance the performance on several knowledge graph tasks like link prediction, triple classification, and multi-hop knowledge base completion.","authors":["Haoyu Wang","Vivek Kulkarni","William Yang Wang"],"keywords":["Knowledge Graph","Contextualized Embeddings"],"pdf_url":"/pdf/5def4d7bb7910beb50c324f1d628ad945b27da0f.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"/session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"Dolores: Deep Contextualized Knowledge Graph Embeddings","youtube":"z4eB1cmcGm4"},"forum":"ajrveGQBl0","id":"10"},{"content":{"TLDR":"We investigate the combined use of ontologies and embeddings in KG refinement task.","abstract":"Knowledge Graphs (KGs) extracted from text sources are often noisy and lead to poor performance in downstream application tasks such as KG-based question answering. While much of the recent activity is focused on addressing the sparsity of KGs by using embeddings for inferring new facts, the issue of cleaning up of noise in KGs through KG refinement task is not as actively studied. Most successful techniques for KG refinement make use of inference rules and reasoning over ontologies. Barring a few exceptions, embeddings do not make use of ontological information, and their performance in KG refinement task is not well understood. In this paper, we present a KG refinement framework called IterefinE which iteratively combines the two techniques \u2013 one which uses ontological information and inferences rules, viz.,PSL-KGI, and the KG embeddings such as ComplEx and ConvE which do not. As a result, IterefinE is able to exploit not only the ontological information to improve the quality of predictions, but also the power of KG embeddings which (implicitly) perform longer chains of reasoning. The IterefinE framework, operates in a co-training mode and results in explicit type-supervised embeddings of the refined KG from PSL-KGI which we call as TypeE-X. Our experiments over a range of KG benchmarks show that the embeddings that we produce are able to reject noisy facts from KG and at the same time infer higher quality new facts resulting in upto 9% improvement of overall weighted F1 score.","authors":["Siddhant Arora","Srikanta Bedathur","Maya Ramanath","Deepak Sharma"],"keywords":["Knowledge graph refinement","embeddings","inference"],"pdf_url":"/pdf/858984f9aa759b64181c7f0d4ae717fc9418b018.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"/session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"IterefinE: Iterative KG Refinement Embeddings using Symbolic Knowledge","youtube":"Sd1ZPMY2XHc"},"forum":"fCQvGMT57w","id":"18"},{"content":{"TLDR":"We use semantic relations associated with mentions to improve fine-grained entity typing.","abstract":"Fine-grained entity typing results can serve as important information for entities while constructing knowledge bases. It is a challenging task due to the use of large tag sets and the requirement of understanding the context.\nWe find that, in some cases, existing neural fine-grained entity typing models may ignore the semantic information in the context that is important for typing. \nTo address this problem, we propose to exploit semantic relations extracted from the sentence to improve the use of context. The used semantic relations are mainly those that are between the mention and the other words or phrases in the sentence. \nWe investigate the use of two types of semantic relations: hypernym relation, and verb-argument relation. Our approach combine the predictions made based on different semantic relations and the predictions of a base neural model to produce the final results. We conduct experiments on two commonly used datasets: FIGER (GOLD) and BBN. Our approach achieves at least 2\\% absolute strict accuracy improvement on both datasets compared with a strong BERT based model.","authors":["Hongliang Dai","Yangqiu Song","Xin Li"],"keywords":["Fine-grained Entity Typing","Hypernym Extraction","Semantic Role Labeling"],"pdf_url":"/pdf/b91e260e0c6b51dbad1183607c61b5490ccb0b49.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"/session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"Exploiting Semantic Relations for Fine-grained Entity Typing","youtube":"1jZrb0T0qR8"},"forum":"BSUYfTada3","id":"6"},{"content":{"TLDR":"We describe a gold standard corpus of protest events that comprise of various local and international sources from various countries in English.","abstract":"We describe a gold standard corpus of protest events that comprise of various local and international sources from various countries in English. The corpus contains document, sentence, and token level annotations. This corpus facilitates creating machine learning models that automatically classify news articles and extract protest event related information, constructing databases which enable comparative social and political science studies. For each news source, the annotation starts on random samples of news articles and continues with samples that are drawn using active learning. Each batch of samples was annotated by two social and political scientists, adjudicated by an annotation supervisor, and was improved by identifying annotation errors semi-automatically. We found that the corpus has the variety and quality to develop and benchmark text classification and event extraction systems in a cross-context setting, which contributes to generalizability and robustness of automated text processing systems. This corpus and the reported results will set the currently lacking common ground in automated protest event collection studies.","authors":["Ali H\u00fcrriyeto\u011flu","Erdem Y\u00f6r\u00fck","Deniz Y\u00fcret","Osman Mutlu","\u00c7a\u011fr\u0131 Yoltar","F\u0131rat Duru\u015fan","Burak G\u00fcrel"],"keywords":["protests","contentious politics","news","text classification","event extraction","social sciences","political sciences","computational social science"],"pdf_url":"/pdf/1bde724acb48562f9d8d8f077e4e0236379e8596.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"/session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Cross-context News Corpus for Protest Events related Knowledge Base Construction","youtube":"0dBDXinNnYA"},"forum":"7NZkNhLCjp","id":"90"},{"content":{"TLDR":"We advance CSK towards a more expressive stage of multifaceted knowledge, and use joint reasoning over all statements to ensure coherence and combat sparsity.","abstract":"Commonsense knowledge (CSK) supports a variety of AI applications, from visual understanding to chatbots. Prior works on acquiring CSK, such as ConceptNet, have compiled statements that associate concepts with properties that hold for most or some of their instances. Each concept and statement is treated in isolation from others, and the only quantitative measure (or ranking) is a confidence score that the statement is valid. This paper aims to overcome these limitations by introducing a multi-faceted model of CSK statements and methods for joint reasoning over sets of inter-related statements. Our model captures four different dimensions of CSK statements: plausibility, typicality, remarkability and salience, with scoring and ranking along each dimension. For example, hyenas drinking water is typical but not salient, whereas hyenas eating carcasses is salient. For reasoning and ranking, we develop a method with soft constraints, to couple the inference over concepts that are related in a taxonomic hierarchy. The reasoning is cast into an integer linear programming (ILP), and we leverage the theory of reduction costs of a relaxed LP to compute informative rankings. Our evaluation shows that we can consolidate existing CSK collections into much cleaner and more expressive knowledge.","authors":["Yohan Chalier","Simon Razniewski","Gerhard Weikum"],"keywords":["Commonsense knowledgebase construction"],"pdf_url":"/pdf/f56e2fd777ea63f87a20c7a84e728c45b2d659de.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"/session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"Joint Reasoning for Multi-Faceted Commonsense Knowledge","youtube":"IMHlhIw4hCA"},"forum":"QnPV72SZVt","id":"2"},{"content":{"TLDR":"Syntactic Question Abstraction and Retrieval for Data-Scarce Semantic Parsing","abstract":"Deep learning approaches to semantic parsing require a large amount of labeled data, but annotating complex logical forms is costly.  Here, we propose SYNTACTIC QUESTION ABSTRACTION & RETRIEVAL (SQAR), a method to build a neural semantic parser that translates a natural language (NL) query to a SQL logical form (LF) with less than 1,000 annotated examples.  SQAR first retrieves a logical pattern from the train data by computing the similarity between NL queries and then grounds a lexical information on the retrieved pattern in order to generate the final LF. We validate SQAR by training models using various small subsets of WikiSQL train data achieving up to 4.9% higher LF accuracy compared to the previous state-of-the-art models on WikiSQL test set.  We also show that by using query-similarity to retrieve logical pattern, SQAR can leverage a paraphrasing dataset achieving up to 5.9% higher LF accuracy compared to the case where SQAR is trained by using only WikiSQL data. In contrast to a simple pattern classification approach, SQAR can generate unseen logical patterns upon the addition of new examples without re-training the model. We also discuss an ideal way to create cost efficient and robust train datasets when the data distribution can be approximated under a data-hungry setting.","authors":["Wonseok Hwang","Jinyeong Yim","Seunghyun Park","Minjoon Seo"],"keywords":["Semantic Parsing","NLIDB","WikiSQL","Question Answering","SQL","Information Retrieval"],"pdf_url":"/pdf/51c7819a764baf76d9c7cac220c7defd01757209.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"/session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Syntactic Question Abstraction and Retrieval for Data-Scarce Semantic Parsing","youtube":"Nh40871cseg"},"forum":"5c_ZmAdVfI","id":"30"},{"content":{"TLDR":"Most knowledge bases so far only contain positive information. We argue for the importance of negative information, and present two methods to mine it.","abstract":"Knowledge bases (KBs), pragmatic collections of knowledge about notable entities, are an important asset in applications such as search, question answering and dialogue. Rooted in a long tradition in knowledge representation, all popular KBs only store positive information, but abstain from taking any stance towards statements not contained in them.\n\nIn this paper, we make the case for explicitly stating interesting statements which are not true. Negative statements would be important to overcome current limitations of question answering, yet due to their potential abundance, any effort towards compiling them needs a tight coupling with ranking. We introduce two approaches towards automatically compiling negative statements. (i) In peer-based statistical inferences, we compare entities with highly related entities in order to derive potential negative statements, which we then rank using supervised and unsupervised features. (ii) In pattern-based query log extraction, we use a pattern-based approach for harvesting search engine query logs. Experimental results show that both approaches hold promising and complementary potential. Along with this paper, we publish the first datasets on interesting negative information, containing over 1.4M statements for 130K popular Wikidata entities.","authors":["Hiba Arnaout","Simon Razniewski","Gerhard Weikum"],"keywords":["information retrieval","knowledge bases","ranking","negation"],"pdf_url":"/pdf/3b6dc13840b1eb2ac65949374068d574bd2bda26.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"/session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"Enriching Knowledge Bases with Interesting Negative Statements","youtube":"Q-C2MbzGXjc"},"forum":"pSLmyZKaS","id":"4"},{"content":{"TLDR":"A novel unsupervised approach to mine domain-dependent variational attributes present in unstructured text and use it to identify entity pairs that are the same and pairs that are variations of each other.","abstract":"Presence of near identical, but distinct, entities called entity variations makes the task of data integration challenging. For example, in the domain of grocery products, variations share the same value for attributes such as brand, manufacturer and product line, but differ in other attributes, called variational attributes, such as package size and color. Identifying variations across data sources is an important task in itself and is crucial for identifying duplicates. However, this task is challenging as the variational attributes are often present as a part of unstructured text and are domain dependent. In this work, we propose our approach, Contrastive entity linkage, to identify both entity pairs that are the same and pairs that are variations of each other. We propose a novel unsupervised approach, VarSpot, to mine domain-dependent variational attributes present in unstructured text. The proposed approach reasons about both similarities and differences between entities and can easily scale to large sources containing millions of entities. We show the generality of our approach by performing experimental evaluation on three different domains. Our approach significantly outperforms state-of-the-art learning-based and rule-based entity linkage systems by up to 4% F1 score when identifying duplicates, and up to 41% when identifying entity variations.","authors":["Varun Embar","Bunyamin Sisman","Hao Wei","Xin Luna Dong","Christos Faloutsos","Lise Getoor"],"keywords":["entity resolution","entity linkage","variations","product linkage","product catalogs"],"pdf_url":"/pdf/328eed7cd685a1682ca22a7847de7bc08995baf9.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"/session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Contrastive Entity Linkage: Mining Variational Attributes from Large Catalogs for Entity Linkage","youtube":"qLv5G-lhLXs"},"forum":"fR44nF03Rb","id":"26"},{"content":{"TLDR":"Retrieving medical information from oncology literature using NLP. ","abstract":"The vast and rapidly expanding volume of biomedical literature makes it difficult for domain experts to keep up with the evidence. In this work, we specifically consider the exponentially growing subarea of genetics in cancer. The need to synthesize and centralize this evidence for dissemination has motivated a team of physicians (with whom this work is a collaboration) to manually construct and maintain a knowledge base that distills key results reported in the literature. This is a laborious process that entails reading through full-text articles to understand the study design, assess study quality, and extract the reported cancer risk estimates associated with particular hereditary cancer genes (i.e., \\emph{penetrance}). In this work, we propose models to automatically surface key elements from full-text cancer genetics articles, with the ultimate aim of expediting the manual workflow currently in place.\n\nWe propose two challenging tasks that are critical for characterizing the findings reported cancer genetics studies: (i) Extracting snippets of text that describe \\emph{ascertainment mechanisms}, which in turn inform whether the population studied may introduce bias owing to deviations from the target population; (ii) Extracting reported risk estimates (e.g., odds or hazard ratios) associated with specific germline mutations. The latter task may be viewed as a joint entity tagging and relation extraction problem. To train models for these tasks, we induce distant supervision over tokens and snippets in full-text articles using the manually constructed knowledge base. We propose and evaluate several model variants, including a transformer-based joint entity and relation extraction model to extract \\texttt{<germline mutation, risk-estimate>} pairs. We observe strong empirical performance, highlighting the practical potential for such models to aid KB construction in this space. We ablate components of our model, observing, e.g., that a joint model for \\texttt{<germline mutation, risk-estimate>} fares substantially better than a pipelined approach. ","authors":["Somin Wadhwa","Kanhua Yin","Kevin S. Hughes","Byron Wallace"],"keywords":["Cancer genetics","biomedical nlp","information extraction","clinical informatics","knowledge base construction"],"pdf_url":"/pdf/948bfb23353a017c1a16b1f435a871e64bc2af38.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"/session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"Semi-Automating Knowledge Base Construction for Cancer Genetics","youtube":"TgJINTJLYJA"},"forum":"EQrvONEwh","id":"50"},{"content":{"TLDR":"Learning to predict relation entailment using both structured and textual information","abstract":"Relations among words and entities are important for semantic understanding of text, but previous work has largely not considered relations between relations, or meta-relations. In this paper, we specifically examine relation entailment, where the existence of one relation can entail the existence of another relation. Relation entailment allows us to construct relation hierarchies, enabling applications in representation learning, question answering, relation extraction, and summarization. To this end, we formally define the new task of predicting relation entailment and construct a dataset by expanding the existing Wikidata relation hierarchy without expensive human intervention. We propose several methods that incorporate both structured and textual information to represent relations for this task. Experiments and analysis demonstrate that this task is challenging, and we provide insights into task characteristics that may form a basis for future work. The dataset and code have been released at https://github.com/jzbjyb/RelEnt.","authors":["Zhengbao Jiang","Jun Araki","Donghan Yu","Ruohong Zhang","Wei Xu","Yiming Yang","Graham Neubig"],"keywords":["relation entailment","structured information","textual information"],"pdf_url":"/pdf/564a1529e2b80aba972e3434a5c2b9c855e155db.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"/session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Learning Relation Entailment with Structured and Textual Information","youtube":"pCmiDtfbbcA"},"forum":"ToTf_MX7Vn","id":"14"},{"content":{"TLDR":"A data-centric domain adaptation framework for  KG question answering for unseen domains","abstract":"Knowledge Graph Simple Question Answering (KGSQA), in its standard form, does not take into account that human-curated question answering training data only cover a small subset of the relations that exist in a Knowledge Graph (KG), or even worse, that new domains covering unseen and rather different to existing domains relations are added to the KG. In this work, we study KGQA for first-order questions in a previously unstudied setting where new, unseen, domains are added during test time. In this setting, question-answer pairs of the new domain do not appear during training, thus making the task more challenging. We propose a data-centric domain adaptation framework that consists of a KGQA system that is applicable to new domains, and a sequence to sequence question generation method that automatically generates question-answer pairs for the new domain. Since the effectiveness of question generation for KGQA can be restricted by the limited lexical variety of the generated questions, we use distant supervision to extract a set of keywords that express each relation of the unseen domain and incorporate those in the question generation method. Experimental results demonstrate that our framework significantly improves over zero-shot baselines and is robust across domains.","authors":["Georgios Sidiropoulos","Nikos Voskarides","Evangelos Kanoulas"],"keywords":["Question Answering","Knowledge Graph","Domain Adaptation"],"pdf_url":"/pdf/80ca76350ab431fb1875b73448f00dca13a0fdb1.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"/session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"Knowledge Graph Simple Question Answering for Unseen Domains","youtube":"HFXRLaYP0OU"},"forum":"Ie2Y94Ty8K","id":"35"},{"content":{"TLDR":"We present DynaPro, a neural RC model that tracks the states of a process's participants using jointly formulated entity-aware and attribute-aware representations. DynaPro achieves SOTA results on two procedural RC datasets: ProPara and NPN-cooking.","abstract":"Procedural texts  often describe processes (e.g., photosynthesis, cooking) that happen over entities (e.g., light, food). In this paper, we introduce an algorithm for procedural reading comprehension by translating the text into a general formalism that represents processes as a sequence of transitions over entity attributes (e.g., location, temperature). Leveraging pre-trained language models, our model obtains entity-aware and attribute-aware representations of the text by joint prediction of entity attributes and their transitions. Our model dynamically obtains contextual encodings of the procedural text exploiting information that is encoded about previous and current states to predict the transition of a certain attribute which can be identified as a spans of texts  or  from a pre-defined set of classes. Moreover, Our model achieves state of the art on two procedural reading comprehension datasets, namely ProPara and npn-Cooking.","authors":["Aida Amini","Antoine Bosselut","Bhavana Dalvi Mishra","Yejin Choi","Hannaneh Hajishirzi"],"keywords":["Reading comprehension","contextual encoding","encoder-decoder architecture","procedural text","entity tracking"],"pdf_url":"/pdf/4ff6430a1bb049f011da2329714748e37d4f5130.pdf","session":[{"calendarId":"poster","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"11:20","link":"/session_d1posters.html","sessionId":"d1posters","start":"10:20","title":"Day 1 Poster Session"}],"title":"Procedural Reading Comprehension with Attribute-Aware Context Flow","youtube":"6KUYVx6wp2c"},"forum":"grnYRcBwjq","id":"63"},{"content":{"TLDR":"We publish a new evaluation benchmark for knowledge graph completion methods where ranking is replaced with actual classification, and show one way to improve knowledge graph embedding models in this new setting.","abstract":"Knowledge base completion (KBC) methods aim at inferring missing facts from the information present in a knowledge base (KB). Such a method thus needs to estimate the likelihood of candidate facts and ultimately to distinguish between true facts and false ones to avoid compromising the KB with untrue information. In the prevailing evaluation paradigm, however, models do not actually decide whether a new fact should be accepted or not but are solely judged on the position of true facts in a likelihood ranking with other candidates. We argue that consideration of binary predictions is essential to reflect the actual KBC quality, and propose a novel evaluation paradigm, designed to provide more transparent model selection criteria for a realistic scenario. We construct the data set FB14k-QAQ with an alternative evaluation data structure: instead of single facts, we use KB queries, i.e., facts where one entity is replaced with a variable, and construct corresponding sets of entities that are correct answers. We randomly remove some of these correct answers from the data set, simulating the realistic scenario of real-world entities missing from a KB. This way, we can explicitly measure a model\u2019s ability to handle queries that have more correct answers in the real world than in the KB, including the special case of queries without any valid answer. The latter especially contrasts the ranking setting. We evaluate a number of state-of-the-art KB embeddings models on our new benchmark. The differences in relative performance between ranking-based and classification-based evaluation that we observe in our experiments confirm our hypothesis that good performance on the ranking task does not necessarily translate to good performance on the actual completion task. Our results motivate future work on KB embedding models with better prediction separability and, as a first step in that direction, we propose a simple variant of TransE that encourages thresholding and achieves a significant improvement in classification F 1 score relative to the original TransE.","authors":["Marina Speranskaya","Martin Schmitt","Benjamin Roth"],"keywords":["knowledge base completion","knowledge graph embedding","classification","ranking"],"pdf_url":"/pdf/a11e6a97c8d61b1e10d72dffde74e70d67bf136e.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"/session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"Ranking vs. Classifying: Measuring Knowledge Base Completion Quality","youtube":"5P5oTKkN9XM"},"forum":"3pcecaCEK-","id":"81"},{"content":{"TLDR":"We study the shortcomings of link prediction evaluation and provide a new task based on triple classification","abstract":"Representing knowledge graphs (KGs) by learning embeddings for entities and relations has led to accurate models for existing KG completion benchmarks. However, due to the open-world assumption of existing KGs, evaluation of KG completion uses ranking metrics and triple classification with negative samples, and is thus unable to directly assess models on the goals of the task: completion. In this paper, we first study the shortcomings of these evaluation metrics. Specifically, we demonstrate that these metrics (1) are unreliable for estimating how calibrated the models are, (2) make strong assumptions that are often violated, and 3) do not sufficiently, and consistently, differentiate embedding methods from each other, or from simpler approaches. To address these issues, we gather a semi-complete KG referred as YAGO3-TC, using a random subgraph from the test and validation data of YAGO3-10, which enables us to compute accurate triple classification accuracy on this data.  Conducting thorough experiments on existing models, we provide new insights and directions for the KG completion research. Along with the dataset and the open source implementation of the models, we also provide a leaderboard for knowledge graph completion that consists of a hidden, and growing, test set, available at https://pouyapez.github.io/yago3-tc/.","authors":["Pouya Pezeshkpour","Yifan Tian","Sameer Singh"],"keywords":["Knowledge Graph Completion","Link prediction","Calibration","Triple Classification"],"pdf_url":"/pdf/d6b0ec116a1c450ea723e6b40f28f05394c3bfb6.pdf","session":[{"calendarId":"poster","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"11:20","link":"/session_d1posters.html","sessionId":"d1posters","start":"10:20","title":"Day 1 Poster Session"}],"title":"Revisiting Evaluation of Knowledge Base Completion Models","youtube":"m9Yqo-wmXQo"},"forum":"1uufzxsxfL","id":"5"},{"content":{"TLDR":"We propose the Graph Hawkes Neural Network for predicting future events on large-scale temporal knowledge graphs.","abstract":"The Hawkes process has become a standard method for modeling self-exciting event sequences with different event types. A recent work has generalized the Hawkes process to a neurally self-modulating multivariate point process, which enables the capturing of more complex and realistic impacts of past events on future events. However, this approach is limited by the number of possible event types, making it impossible to model the dynamics of evolving graph sequences, where each possible link between two nodes can be considered as an event type. The number of event types increases even further when links are directional and labeled. To address this issue, we propose the Graph Hawkes Neural Network that can capture the dynamics of evolving graph sequences and can predict the occurrence of a fact in a future time instance. Extensive experiments on large-scale temporal multi-relational databases, such as temporal knowledge graphs, demonstrate the effectiveness of our approach.","authors":["Zhen Han","Yunpu Ma","Yuyi Wang","Stephan Gu\u0308nnemann","Volker Tresp"],"keywords":["Hawkes process","dynamic graphs","temporal knowledge graphs","point processes."],"pdf_url":"/pdf/beb949d7d2e017e058d2cf01c19d7c0160e64979.pdf","session":[{"calendarId":"poster","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"11:20","link":"/session_d1posters.html","sessionId":"d1posters","start":"10:20","title":"Day 1 Poster Session"}],"title":"Graph Hawkes Neural Network for Forecasting on Temporal Knowledge Graphs","youtube":"xzLcP2Jg9sY"},"forum":"kXVazet_cB","id":"20"},{"content":{"TLDR":"We present an outcome explanation engine for Factorization based KBC","abstract":"State-of-the-art models for Knowledge Base Completion (KBC) are based on tensor factorization (TF), e.g, DistMult, ComplEx. While they produce good results, they cannot expose any rationale behind their predictions, potentially reducing the trust of a user in the model. Previous works have explored creating an inherently explainable model, e.g. Neural Theorem Proving (NTP), DeepPath, MINERVA, but explainability comes at the cost of performance. Others have tried to create an auxiliary explainable model having high fidelity with the underlying TF model, but unfortunately, they do not scale on large KBs such as FB15k and YAGO.\u00a0In this work, we propose OxKBC -- an Outcome eXplanation engine for KBC, which provides a post-hoc explanation for every triple inferred by an (uninterpretable) factorization based model. It first augments the underlying Knowledge Graph by introducing weighted edges between entities based on their similarity given by the underlying model. In the augmented graph, it defines a notion of human-understandable explanation paths along with a language to generate them. Depending on the edges, the paths are aggregated into second-order templates for further selection. The best template with its grounding is then selected by a neural selection module that is trained with minimal supervision by a novel loss function. Experiments over Mechanical Turk demonstrate that users find our explanations more trustworthy compared to rule mining.","authors":["Yatin Nandwani","Ankesh Gupta","Aman Agrawal","Mayank Singh Chauhan","Parag Singla","Mausam"],"keywords":["xai","kbc","templates","outcome explanation","templates"],"pdf_url":"/pdf/45344fe9807a05403de6f6b8cce593d126b9e3b4.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"/session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"OxKBC: Outcome Explanation for Factorization Based Knowledge Base Completion","youtube":"n7PXsp1WZBI"},"forum":"nqYhFwaUj","id":"55"},{"content":{"TLDR":"We propose TransINT, a novel and interpretable KG embedding method that isomorphically preserves the implication ordering among relations in the embedding space in an explainable, robust, and geometrically coherent way.","abstract":"Knowledge Graphs (KG), composed of entities and relations, provide a structured representation of knowledge. For easy access to statistical approaches on relational data, multiple methods to embed a KG into f(KG) \u2208 R^d have been introduced. We propose TransINT, a novel and interpretable KG embedding method that isomorphically preserves the implication ordering among relations in the embedding space. Given implication rules, TransINT maps set of entities (tied by a relation) to continuous sets of vectors that are inclusion-ordered isomorphically to relation implications. With a novel parameter sharing scheme, TransINT enables automatic training on missing but implied facts without rule grounding. On a benchmark dataset, we outperform the best existing state-of-the-art rule integration embedding methods with significant margins in link Prediction and triple Classification. The angles between the continuous sets embedded by TransINT provide an interpretable way to mine semantic relatedness and implication rules among relations.","authors":["So Yeon Min","Preethi Raghavan","Peter Szolovits"],"keywords":["Knowledge Graph Embedding","Isomorphism","Rules","Common Sense","Implication Rules","Knowledge Graph","Isomorphic Embedding","Semantics Mining","Rule Mining","TransH"],"pdf_url":"/pdf/e9d03f36d42f9d263006c982f5e0a082e17e5782.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"/session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"TransINT: Embedding Implication Rules in Knowledge Graphs with Isomorphic Intersections of Linear Subspaces","youtube":"HS8TJGpLeJs"},"forum":"shkmWLRBXH","id":"87"},{"content":{"TLDR":"Extending Box embeddings to model multiple relations","abstract":"Learning representations for hierarchical and multi-relational knowledge has emerged as an active area of research. Box Embeddings  [Vilnis et al., 2018, Li et al., 2019] represent concepts with hyperrectangles in $n$-dimensional space and are shown to be capable of modeling tree-like structures efficiently by training on a large subset of the transitive closure of the WordNet hypernym graph. In this work, we evaluate the capability of box embeddings to learn the transitive closure of a tree-like hierarchical relation graph with far fewer edges from the transitive closure. Box embeddings are not restricted to tree-like structures, however, and we demonstrate this by modeling the WordNet meronym graph, where nodes may have multiple parents. We further propose a method for modeling multiple relations jointly in a single embedding space using box embeddings. In all cases, our proposed method outperforms or is at par with all other embedding methods.","authors":["Dhruvesh Patel","Shib Sankar Dasgupta","Michael Boratko","Xiang Li","Luke Vilnis","Andrew McCallum"],"keywords":["embeddings","order embeddings","knowledge graph embedding","relational learning","hyperbolic entailment cones","knowledge graphs","transitive relations"],"pdf_url":"/pdf/b6de1849f0ca7a9ed23483c4d9420dccbd365f7b.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"/session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Representing Joint Hierarchies with Box Embeddings","youtube":"yqP8wjMocAs"},"forum":"J246NSqR_l","id":"91"},{"content":{"TLDR":"We present Sampo, an unsupervised technique for creating a knowledge base of opinions expressed in reviews and their implications.","abstract":"Knowledge bases (KBs) have long been the backbone of many real-world applications and services. There are many KB construction (KBC) methods that can extract factual information, where relationships between entities are explicitly stated in text. However, they cannot model implications between opinions which are abundant in user-generated text such as reviews and often have to be mined. Our goal is to develop a technique to build KBs that can capture both opinions and their implications. Since it can be expensive to obtain training data to learn to extract implications for each new domain of reviews, we propose an unsupervised KBC system, Sampo, Specifically, Sampo is tailored to build KBs for domains where many reviews on the same domain are available. We generate KBs for 20 different domains using Sampo and manually evaluate KBs for 6 domains. Our experiments show that KBs generated using Sampo capture information otherwise missed by other KBC methods. Specifically, we show that our KBs can provide additional training data to fine-tune language models that are used for downstream tasks such as review comprehension.","authors":["Nikita Bhutani","Aaron Traylor","Chen Chen","Xiaolan Wang","Behzad Golshan","Wang-Chiew Tan"],"keywords":["knowledge base construction","matrix factorization","mining opinion implications"],"pdf_url":"/pdf/72b83268b1c0e11b44f8581c355f8cb5ac6f6322.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"/session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Sampo: Unsupervised Knowledge Base Construction for Opinions and Implications","youtube":"IbB51-WH9PU"},"forum":"YN8fkglNA","id":"54"},{"content":{"TLDR":"Predicting hierarchies of institutions by modeling set operations over tokens","abstract":"The hierarchical structure of research organizations plays a pivotal role in science of science research as well as in tools that track the research achievements and output. However, this structure is not consistently documented for all institutions in the world, motivating the need for automated construction methods. In this paper, we present a new task and model for predicting sub-institution/super-institution relationships based on their string names. The crux of our model is that it leverages learned, permutation invariant representations of various token subsets of institution name strings. Our model outperforms or matches non-set-based models and baselines. We also create a dataset for training and evaluating models for this task based on the publicly available relationships in the Global Research Identifier Database.","authors":["Derek Tam","Nicholas Monath","Ari Kobren","Andrew McCallum"],"keywords":["Hierarchies","Sets","Transformers","Institutions"],"pdf_url":"/pdf/aca5866d5727cd021e5a9b1e1bdcaa06555b5ad6.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"/session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Predicting Institution Hierarchies with Set-based Models","youtube":"oYSgWGoYbJM"},"forum":"pJg1LahGc0","id":"49"},{"content":{"TLDR":"We propose a scalable method to construct  the large-scale eventuality entailment graph with high precision. ","abstract":"Computational and cognitive studies suggest that the abstraction of eventualities (activities, states, and events) is crucial for humans to understand daily eventualities. In this paper, we propose a scalable approach to model the entailment relations between eventualities (\"eat an apple'' entails ''eat fruit''). As a result, we construct a large-scale eventuality entailment graph (EEG), which has 10 million eventuality nodes and 103 million entailment edges. Detailed experiments and analysis demonstrate the effectiveness of the proposed approach and quality of the resulting knowledge graph. Our datasets and code are available at https://github.com/HKUST-KnowComp/ASER-EEG.","authors":["Changlong Yu","Hongming Zhang","Yangqiu Song","Wilfred Ng","Lifeng Shang"],"keywords":["eventuality knowledge graph","entailment graph","commonsense reasoning"],"pdf_url":"/pdf/888876ed59decc02bf40752432660b526af5d17c.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"/session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Enriching Large-Scale Eventuality Knowledge Graph with Entailment Relations","youtube":"TIsZAGYeSyQ"},"forum":"-oXaOxy6up","id":"69"},{"content":{"TLDR":"Learn to answer a query about an entity by gathering reasoning paths from other similar entities in the Knowledge Base","abstract":"We present a surprisingly simple yet accurate approach to reasoning in knowledge graphs (KGs) that requires \\emph{no training}, and is reminiscent of case-based reasoning in classical artificial intelligence (AI). \nConsider the task of finding a target entity given a source entity and a binary relation.  \nOur approach finds multiple \\textit{graph path patterns} that connect similar source entities through the given relation, and looks for pattern matches starting from the query source.  \nUsing our method, we obtain new state-of-the-art accuracy, outperforming all previous models, on NELL-995 and FB-122. \nWe also demonstrate that our model is robust in low data settings, outperforming recently proposed meta-learning approaches.\n","authors":["Rajarshi Das","Ameya Godbole","Shehzaad Dhuliawala","Manzil Zaheer","Andrew McCallum"],"keywords":["case based reasoning","non-parametric reasoning","knowledge base completion"],"pdf_url":"/pdf/532b85a26bd9d5801b778f3cfbd1be490ba2cfaf.pdf","session":[{"calendarId":"poster","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"11:20","link":"/session_d1posters.html","sessionId":"d1posters","start":"10:20","title":"Day 1 Poster Session"}],"title":"A Simple Approach to Case-Based Reasoning in Knowledge Bases","youtube":"fC7fNpVtKis"},"forum":"AEY9tRqlU7","id":"48"}]
