[{"UID":"d1","date":"2020-06-22","highlighted":[{"content":{"TLDR":"","abstract":"When pre-trained on large unsupervised textual corpora, language models are able to store and retrieve factual knowledge to some extent, making it possible to use them directly for zero-shot cloze-style question answering. However, storing factual knowledge in a fixed number of weights of a language model clearly has limitations. Previous approaches have successfully provided access to information outside the model weights using supervised architectures that combine an information retrieval system with a machine reading component. In this paper, we go one step further and integrate information from a retrieval system with a pre-trained language model in a purely unsupervised way. We report that augmenting pre-trained language models in this way dramatically improves performance and that it is competitive with a supervised machine reading baseline without requiring any supervised training. Furthermore, processing query and context with different segment tokens allows BERT to utilize its Next Sentence Prediction pre-trained classifier to determine whether the context is relevant or not, substantially improving BERT's zero-shot cloze-style question-answering performance and making its predictions robust to noisy contexts.","authors":["Fabio Petroni","Patrick Lewis","Aleksandra Piktus","Tim Rockt\u00e4schel","Yuxiang Wu","Alexander H. Miller","Sebastian Riedel"],"keywords":[],"pdf_url":"/pdf/9bc20e38dd19b074f9d563a2d1bf5ecbc90d6294.pdf","session":[{"calendarId":"poster","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"11:20","link":"session_d1posters.html","sessionId":"d1posters","start":"10:20","title":"Day 1 Poster Session"}],"title":"How Context Affects Language Models' Factual Predictions","youtube":"2BURBZF5GmY"},"forum":"025X0zPfn","id":"21"},{"content":{"TLDR":"We present DynaPro, a neural RC model that tracks the states of a process's participants using jointly formulated entity-aware and attribute-aware representations. DynaPro achieves SOTA results on two procedural RC datasets: ProPara and NPN-cooking.","abstract":"Procedural texts  often describe processes (e.g., photosynthesis, cooking) that happen over entities (e.g., light, food). In this paper, we introduce an algorithm for procedural reading comprehension by translating the text into a general formalism that represents processes as a sequence of transitions over entity attributes (e.g., location, temperature). Leveraging pre-trained language models, our model obtains entity-aware and attribute-aware representations of the text by joint prediction of entity attributes and their transitions. Our model dynamically obtains contextual encodings of the procedural text exploiting information that is encoded about previous and current states to predict the transition of a certain attribute which can be identified as a spans of texts  or  from a pre-defined set of classes. Moreover, Our model achieves state of the art on two procedural reading comprehension datasets, namely ProPara and npn-Cooking.","authors":["Aida Amini","Antoine Bosselut","Bhavana Dalvi Mishra","Yejin Choi","Hannaneh Hajishirzi"],"keywords":["Reading comprehension","contextual encoding","encoder-decoder architecture","procedural text","entity tracking"],"pdf_url":"/pdf/4ff6430a1bb049f011da2329714748e37d4f5130.pdf","session":[{"calendarId":"poster","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"11:20","link":"session_d1posters.html","sessionId":"d1posters","start":"10:20","title":"Day 1 Poster Session"}],"title":"Procedural Reading Comprehension with Attribute-Aware Context Flow","youtube":"6KUYVx6wp2c"},"forum":"grnYRcBwjq","id":"63"},{"content":{"TLDR":"We study the shortcomings of link prediction evaluation and provide a new task based on triple classification","abstract":"Representing knowledge graphs (KGs) by learning embeddings for entities and relations has led to accurate models for existing KG completion benchmarks. However, due to the open-world assumption of existing KGs, evaluation of KG completion uses ranking metrics and triple classification with negative samples, and is thus unable to directly assess models on the goals of the task: completion. In this paper, we first study the shortcomings of these evaluation metrics. Specifically, we demonstrate that these metrics (1) are unreliable for estimating how calibrated the models are, (2) make strong assumptions that are often violated, and 3) do not sufficiently, and consistently, differentiate embedding methods from each other, or from simpler approaches. To address these issues, we gather a semi-complete KG referred as YAGO3-TC, using a random subgraph from the test and validation data of YAGO3-10, which enables us to compute accurate triple classification accuracy on this data.  Conducting thorough experiments on existing models, we provide new insights and directions for the KG completion research. Along with the dataset and the open source implementation of the models, we also provide a leaderboard for knowledge graph completion that consists of a hidden, and growing, test set, available at https://pouyapez.github.io/yago3-tc/.","authors":["Pouya Pezeshkpour","Yifan Tian","Sameer Singh"],"keywords":["Knowledge Graph Completion","Link prediction","Calibration","Triple Classification"],"pdf_url":"/pdf/d6b0ec116a1c450ea723e6b40f28f05394c3bfb6.pdf","session":[{"calendarId":"poster","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"11:20","link":"session_d1posters.html","sessionId":"d1posters","start":"10:20","title":"Day 1 Poster Session"}],"title":"Revisiting Evaluation of Knowledge Base Completion Models","youtube":"m9Yqo-wmXQo"},"forum":"1uufzxsxfL","id":"5"},{"content":{"TLDR":"We propose the Graph Hawkes Neural Network for predicting future events on large-scale temporal knowledge graphs.","abstract":"The Hawkes process has become a standard method for modeling self-exciting event sequences with different event types. A recent work has generalized the Hawkes process to a neurally self-modulating multivariate point process, which enables the capturing of more complex and realistic impacts of past events on future events. However, this approach is limited by the number of possible event types, making it impossible to model the dynamics of evolving graph sequences, where each possible link between two nodes can be considered as an event type. The number of event types increases even further when links are directional and labeled. To address this issue, we propose the Graph Hawkes Neural Network that can capture the dynamics of evolving graph sequences and can predict the occurrence of a fact in a future time instance. Extensive experiments on large-scale temporal multi-relational databases, such as temporal knowledge graphs, demonstrate the effectiveness of our approach.","authors":["Zhen Han","Yunpu Ma","Yuyi Wang","Stephan Gu\u0308nnemann","Volker Tresp"],"keywords":["Hawkes process","dynamic graphs","temporal knowledge graphs","point processes."],"pdf_url":"/pdf/beb949d7d2e017e058d2cf01c19d7c0160e64979.pdf","session":[{"calendarId":"poster","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"11:20","link":"session_d1posters.html","sessionId":"d1posters","start":"10:20","title":"Day 1 Poster Session"}],"title":"Graph Hawkes Neural Network for Forecasting on Temporal Knowledge Graphs","youtube":"xzLcP2Jg9sY"},"forum":"kXVazet_cB","id":"20"},{"content":{"TLDR":"Learn to answer a query about an entity by gathering reasoning paths from other similar entities in the Knowledge Base","abstract":"We present a surprisingly simple yet accurate approach to reasoning in knowledge graphs (KGs) that requires \\emph{no training}, and is reminiscent of case-based reasoning in classical artificial intelligence (AI). \nConsider the task of finding a target entity given a source entity and a binary relation.  \nOur approach finds multiple \\textit{graph path patterns} that connect similar source entities through the given relation, and looks for pattern matches starting from the query source.  \nUsing our method, we obtain new state-of-the-art accuracy, outperforming all previous models, on NELL-995 and FB-122. \nWe also demonstrate that our model is robust in low data settings, outperforming recently proposed meta-learning approaches.\n","authors":["Rajarshi Das","Ameya Godbole","Shehzaad Dhuliawala","Manzil Zaheer","Andrew McCallum"],"keywords":["case based reasoning","non-parametric reasoning","knowledge base completion"],"pdf_url":"/pdf/532b85a26bd9d5801b778f3cfbd1be490ba2cfaf.pdf","session":[{"calendarId":"poster","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"11:20","link":"session_d1posters.html","sessionId":"d1posters","start":"10:20","title":"Day 1 Poster Session"}],"title":"A Simple Approach to Case-Based Reasoning in Knowledge Bases","youtube":"fC7fNpVtKis"},"forum":"AEY9tRqlU7","id":"48"}],"interactive_sessions":[{"calendarId":"poster","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"11:20","link":"session_d1posters.html","sessionId":"d1posters","start":"10:20","title":"Day 1 Poster Session"},{"additionalZoomIds":[79182706778],"calendarId":"hallway","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"13:30","link":"session_d1hallway.html","sessionId":"d1hallway","start":"12:30","title":"Day 1 Hallway Chats"}],"name":"Day 1","sessions":[{"calendarId":"---","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"08:20","link":"","sessionId":"opening","start":"08:00","title":"Opening Remarks"},{"calendarId":"invited","chatSession":"d1hallway","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"08:55","link":"speaker_wcohen.html","sessionId":"wcohen","start":"08:20","title":"Invited: William Cohen"},{"calendarId":"invited","chatSession":"d1hallway","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"09:30","link":"speaker_jleskovec.html","sessionId":"jleskovec","start":"08:55","title":"Invited: Jure Leskovec"},{"calendarId":"talks","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"10:20","link":"session_d1posters.html","sessionId":"d1talks","start":"09:30","title":"Day 1 Lightning Talks"},{"calendarId":"poster","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"11:20","link":"session_d1posters.html","sessionId":"d1posters","start":"10:20","title":"Day 1 Poster Session"},{"calendarId":"invited","chatSession":"d1hallway","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"11:55","link":"speaker_jtaylor.html","sessionId":"jtaylor","start":"11:20","title":"Invited: Jamie Taylor"},{"calendarId":"invited","chatSession":"d1hallway","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"12:30","link":"speaker_ldong.html","sessionId":"ldong","start":"11:55","title":"Invited: Xin Luna Dong"},{"additionalZoomIds":[79182706778],"calendarId":"hallway","date":"2020-06-22","day_name":"Day 1","day_uid":"d1","end":"13:30","link":"session_d1hallway.html","sessionId":"d1hallway","start":"12:30","title":"Day 1 Hallway Chats"}],"speakers":[{"UID":"wcohen","abstract":"KB completion can be viewed as answering structured queries against a KB. Since answering the queries requires more than simply retrieving known facts, answering these queries requires some non-trivial processing, and hence is broadly similar to logical inference in a conventional symbolic KB. KB question-answering (KBQA) also answers queries against a KB, but in this case the queries are unstructured text queries. So both KBQA and KBC use some analog of \"reasoning\" over a KB. This raises the question: what can we learn about KBQA and KBC from the classical AI subfield of knowledge representation (KR)? In KR the central question is how to represent knowledge in a form that supports efficient, expressive reasoning. In my talk I will try to revisit this question in the context of modern neural learning methods, and tie the themes explored in classical KR to recently-proposed methods for KBC and KBQA.","institution":"Google AI","speaker":"William Cohen","thumbnail":"wcohen.jfif","title":"KR for KBQA and KBC","url":"http://www.cs.cmu.edu/~wcohen/"},{"UID":"jleskovec","abstract":"Learning low-dimensional embeddings of knowledge graphs is a powerful approach for predicting unobserved or missing relations between entities. However, an open challenge in this area is developing techniques that can go beyond single edge prediction and handle more complex multi-hop logical queries, which might involve multiple unobserved edges, entities, and variables. In this talk we present a framework to efficiently answer multi-hop logical queries on knowledge graphs. Our main insight is that queries can be embedded as boxes (i.e., hyper-rectangles), where a set of points inside the box corresponds to a set of answer entities of the query. We show that conjunctions and disjunctions can be naturally represented as intersections/unions of boxes. We demonstrate the effectiveness of our approach on large KGs and show its robustness in the presence of noise and missing relations.","bio":"Jure Leskovec is Associate Professor of Computer Science at Stanford University, Chief Scientist at Pinterest, and investigator at Chan Zuckerberg Biohub. His research focuses on machine learning and data mining with graphs, a general language for describing social, technological and biological systems. Computation over massive data is at the heart of his research and has applications in computer science, social sciences, marketing, and biomedicine. This research has won several awards including a Lagrange Prize, Microsoft Research Faculty Fellowship, the Alfred P. Sloan Fellowship, and numerous best paper and test of time awards. Leskovec received his bachelor's degree in computer science from University of Ljubljana, Slovenia, PhD in machine learning from Carnegie Mellon University and postdoctoral training at Cornell University.","institution":"Stanford","speaker":"Jure Leskovec","thumbnail":"jleskovec.jpg","title":"Representation Learning for Logical Reasoning in Knowledge Graphs","url":"https://cs.stanford.edu/people/jure/"},{"UID":"jtaylor","abstract":"Google's Knowledge Graph is a durable collection of entities.  As new things are learned over time about an entity, those \"facts\" are added to the entity. This long term accumulation of knowledge is central to the value of KG. To make this growth strategy work, the entities must be easily distinguishable from one another and stable in what they represent.  But how should the boundaries between each entity be determined?  Moreover, what is the right granularity of categories and relations that should be applied to these entities?  There are many options for how the world could be cleaved ontologically, but experience with a large stable knowledge graph has shown that pragmatically some criteria may matter more than others. And yet, in some cases, the decision might not be as important as we thought.","bio":"Jamie manages the Schema Team for Google\u2019s Knowledge Graph. The team\u2019s responsibilities include extending KG\u2019s underlying semantic representation, growing coverage of the ontology and enforcing semantic policy. He joined Google following the acquisition of Metaweb Technologies where he was the Minister of Information, helping organize data in Freebase and evangelizing semantic representation to web developers. Prior to Metaweb, Jamie worked in enterprise software as CTO of Determine Software and before that started one of the first ISPs in San Francisco. He is co-author of the O\u2019Reilly book, \u201cProgramming the Semantic Web.\u201d Jamie has a PhD from Harvard University and earned his bachelor\u2019s degree from Colorado College.","institution":"Google","speaker":"Jamie Taylor","thumbnail":"jtaylor.jpg","title":"A long view on Identity","url":"https://www.linkedin.com/in/jamie-taylor-853953/"},{"UID":"ldong","abstract":"Knowledge graphs have been used to support a wide range of applications and enhance search and QA for Google, Amazon Alexa, etc. However, we often miss long-tail knowledge, including unpopular entities, unpopular relations, and unpopular verticals. In this talk we describe our efforts in harvesting knowledge from semi-structured websites, which are often populated according to some templates using vast volume of data stored in underlying databases. We describe our AutoCeres ClosedIE system, which improves the accuracy of fully automatic knowledge extraction from 60%+ of state-of-the-art to 90%+ on semi-structured data. We also describe OpenCeres, the first ever OpenIE system on semi-structured data, that is able to identify new relations not readily included in existing ontologies. In addition, we describe our other efforts in ontology alignment, entity linkage, graph mining, and QA, that allow us to best leverage the knowledge we extract for search and QA.","bio":"Xin Luna Dong is a Principal Scientist at Amazon, leading the efforts of constructing Amazon Product Knowledge Graph. She was one of the major contributors to the Google Knowledge Vault project, and has led the Knowledge-based Trust project, which is called the \u201cGoogle Truth Machine\u201d by Washington\u2019s Post. She has co-authored book \u201cBig Data Integration\u201d, was awarded ACM Distinguished Member, VLDB Early Career Research Contribution Award for \"advancing the state of the art of knowledge fusion\", and Best Demo award in Sigmod 2005. She serves in VLDB endowment and PVLDB advisory committee, and is a PC co-chair for VLDB 2021, ICDE Industry 2019, VLDB Tutorial 2019, Sigmod 2018 and WAIM 2015.\u200b\u200b","institution":"Amazon","speaker":"Xin Luna Dong","thumbnail":"ldong.jpeg","title":"Harvesting knowledge from the semi-structured web","url":"http://lunadong.com/"}],"webinar":{"UID":95610670073,"link":"https://uci.zoom.us/j/95610670073","meeting_password":"none"},"youtube":"JsB4T35We0w"},{"UID":"d2","date":"2020-06-23","highlighted":[{"content":{"TLDR":"","abstract":"Given a set of common concepts like {apple (noun), pick (verb), tree (noun)},  humans find it easy to write a sentence describing a grammatical and logically coherent scenario that covers these concepts,  for example, {a boy picks an apple from a tree''}.  The process of generating these sentences requires humans to use commonsense knowledge. We denote this ability as generative commonsense reasoning. Recent work in commonsense reasoning has focused mainly on discriminating the most plausible scenes from distractors via natural language understanding (NLU) settings such as multi-choice question answering. However, generative commonsense reasoning is a relatively unexplored research area, primarily due to the lack of a specialized benchmark dataset.\n\nIn this paper, we present a constrained natural language generation (NLG) dataset, named CommonGen, to explicitly challenge machines in generative commonsense reasoning. It consists of 30k concept-sets with human-written sentences as references. Crowd-workers were also asked to write the rationales (i.e. the commonsense facts) used for generating the sentences in the development and test sets. We conduct experiments on a variety of generation models with both automatic and human evaluation. Experimental results show that there is still a large gap between the current state-of-the-art pre-trained model, UniLM, and human performance.","authors":["Bill Yuchen Lin","Ming Shen","Wangchunshu Zhou","Pei Zhou","Chandra Bhagavatula","Yejin Choi","Xiang Ren"],"keywords":[],"pdf_url":"","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning","youtube":"WwjyXDuoPtk"},"forum":"yuD2q50HWv","id":"75"},{"content":{"TLDR":"We propose a reliable way to generate citation field extraction dataset from BibTeX. Training models on our dataset achieves new SoTa on UMass CFE dataset.","abstract":"Accurate parsing of citation reference strings is crucial to automatically construct scholarly databases such as Google Scholar or Semantic Scholar. Citation field extraction (CFE) is precisely this task---given a reference label which tokens refer to the authors, venue, title,  editor, journal, pages, etc. Most methods for CFE are supervised and rely on training from labeled datasets that are quite small compared to the great variety of reference formats. BibTeX, the widely used reference management tool, provides a natural method to automatically generate and label training data for CFE. In this paper, we describe a technique for using BibTeX to generate, automatically, a large-scale 41M labeled strings), labeled dataset, that is four orders of magnitude larger than the current largest CFE dataset, namely the UMass Citation Field Extraction dataset [Anzaroot and McCallum, 2013]. We experimentally demonstrate how our dataset can be used to improve the performance of the UMass CFE using a RoBERTa-based [Liu et al., 2019] model. In comparison to previous SoTA, we achieve a 24.48% relative error reduction, achieving span level F1-scores of 96.3%.","authors":["Dung Thai","Zhiyang Xu","Nicholas Monath","Boris Veytsman","Andrew McCallum"],"keywords":["sequence labeling","information extraction","auto-generated dataset"],"pdf_url":"/pdf/c2006bb2e33b09f6f4e0fa73c0c728119982dec0.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Using BibTeX to Automatically Generate Labeled Data for Citation Field Extraction","youtube":"J-vpIctDLms"},"forum":"OnUd3hf3o3","id":"85"},{"content":{"TLDR":"We achieve state of the art on CoNLL and TAC-KBP 2010 with a four layer transformer","abstract":"In this work, we present an entity linking model which combines a Transformer architecture with large scale pretraining from Wikipedia links.  Our model achieves the state-of-the-art on two commonly used entity linking datasets:  96.7% on CoNLL and 94.9% on TAC-KBP. We present detailed analyses to understand what design choices are important for entity linking, including choices of negative entity candidates, Transformer architecture, and input perturbations.  Lastly, we present promising results on more challenging settings such as end-to-end entity linking and entity linking without in-domain training data","authors":["Thibault F\u00e9vry","Nicholas FitzGerald","Livio Baldini Soares","Tom Kwiatkowski"],"keywords":["Entity linking","Pre-training","Wikification"],"pdf_url":"/pdf/98236ce89e5e6d8f6cfd7e77a550638d7470d7d6.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Empirical Evaluation of Pretraining Strategies for Supervised Entity Linking","youtube":"qteiBwBdR24"},"forum":"iHXV8UGYyL","id":"83"},{"content":{"TLDR":"Syntactic Question Abstraction and Retrieval for Data-Scarce Semantic Parsing","abstract":"Deep learning approaches to semantic parsing require a large amount of labeled data, but annotating complex logical forms is costly.  Here, we propose SYNTACTIC QUESTION ABSTRACTION & RETRIEVAL (SQAR), a method to build a neural semantic parser that translates a natural language (NL) query to a SQL logical form (LF) with less than 1,000 annotated examples.  SQAR first retrieves a logical pattern from the train data by computing the similarity between NL queries and then grounds a lexical information on the retrieved pattern in order to generate the final LF. We validate SQAR by training models using various small subsets of WikiSQL train data achieving up to 4.9% higher LF accuracy compared to the previous state-of-the-art models on WikiSQL test set.  We also show that by using query-similarity to retrieve logical pattern, SQAR can leverage a paraphrasing dataset achieving up to 5.9% higher LF accuracy compared to the case where SQAR is trained by using only WikiSQL data. In contrast to a simple pattern classification approach, SQAR can generate unseen logical patterns upon the addition of new examples without re-training the model. We also discuss an ideal way to create cost efficient and robust train datasets when the data distribution can be approximated under a data-hungry setting.","authors":["Wonseok Hwang","Jinyeong Yim","Seunghyun Park","Minjoon Seo"],"keywords":["Semantic Parsing","NLIDB","WikiSQL","Question Answering","SQL","Information Retrieval"],"pdf_url":"/pdf/51c7819a764baf76d9c7cac220c7defd01757209.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Syntactic Question Abstraction and Retrieval for Data-Scarce Semantic Parsing","youtube":"Nh40871cseg"},"forum":"5c_ZmAdVfI","id":"30"},{"content":{"TLDR":"We describe a gold standard corpus of protest events that comprise of various local and international sources from various countries in English.","abstract":"We describe a gold standard corpus of protest events that comprise of various local and international sources from various countries in English. The corpus contains document, sentence, and token level annotations. This corpus facilitates creating machine learning models that automatically classify news articles and extract protest event related information, constructing databases which enable comparative social and political science studies. For each news source, the annotation starts on random samples of news articles and continues with samples that are drawn using active learning. Each batch of samples was annotated by two social and political scientists, adjudicated by an annotation supervisor, and was improved by identifying annotation errors semi-automatically. We found that the corpus has the variety and quality to develop and benchmark text classification and event extraction systems in a cross-context setting, which contributes to generalizability and robustness of automated text processing systems. This corpus and the reported results will set the currently lacking common ground in automated protest event collection studies.","authors":["Ali H\u00fcrriyeto\u011flu","Erdem Y\u00f6r\u00fck","Deniz Y\u00fcret","Osman Mutlu","\u00c7a\u011fr\u0131 Yoltar","F\u0131rat Duru\u015fan","Burak G\u00fcrel"],"keywords":["protests","contentious politics","news","text classification","event extraction","social sciences","political sciences","computational social science"],"pdf_url":"/pdf/1bde724acb48562f9d8d8f077e4e0236379e8596.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Cross-context News Corpus for Protest Events related Knowledge Base Construction","youtube":"0dBDXinNnYA"},"forum":"7NZkNhLCjp","id":"90"},{"content":{"TLDR":"We present Sampo, an unsupervised technique for creating a knowledge base of opinions expressed in reviews and their implications.","abstract":"Knowledge bases (KBs) have long been the backbone of many real-world applications and services. There are many KB construction (KBC) methods that can extract factual information, where relationships between entities are explicitly stated in text. However, they cannot model implications between opinions which are abundant in user-generated text such as reviews and often have to be mined. Our goal is to develop a technique to build KBs that can capture both opinions and their implications. Since it can be expensive to obtain training data to learn to extract implications for each new domain of reviews, we propose an unsupervised KBC system, Sampo, Specifically, Sampo is tailored to build KBs for domains where many reviews on the same domain are available. We generate KBs for 20 different domains using Sampo and manually evaluate KBs for 6 domains. Our experiments show that KBs generated using Sampo capture information otherwise missed by other KBC methods. Specifically, we show that our KBs can provide additional training data to fine-tune language models that are used for downstream tasks such as review comprehension.","authors":["Nikita Bhutani","Aaron Traylor","Chen Chen","Xiaolan Wang","Behzad Golshan","Wang-Chiew Tan"],"keywords":["knowledge base construction","matrix factorization","mining opinion implications"],"pdf_url":"/pdf/72b83268b1c0e11b44f8581c355f8cb5ac6f6322.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Sampo: Unsupervised Knowledge Base Construction for Opinions and Implications","youtube":"IbB51-WH9PU"},"forum":"YN8fkglNA","id":"54"},{"content":{"TLDR":"We study entity linking for Chinese news comment and propose a novel attention based method to detect relevant context and supporting entities from reference articles.","abstract":"Automatic identification of mentioned entities in social media posts facilitates quick digestion of trending topics and popular opinions. Nonetheless, this remains a challenging task due to limited context and diverse name variations. In this paper, we study the problem of entity linking for Chinese news comments given mentions' spans. We hypothesize that comments often refer to entities in the corresponding news article, as well as topics involving the entities. We therefore propose a novel model, XREF, that leverages attention mechanisms to (1) pinpoint relevant context within comments, and (2) detect supporting entities from the news article. To improve training, we make two contributions: (a) we propose a supervised attention loss in addition to the standard cross entropy, and (b) we develop a weakly supervised training scheme to utilize the large-scale unlabeled corpus. Two new datasets in entertainment and product domains are collected and annotated for experiments. Our proposed method outperforms previous methods on both datasets. ","authors":["Xinyu Hua","Lei Li","Lifeng Hua","Lu Wang"],"keywords":["Entity Linking","Chinese social media","Data Augmentation"],"pdf_url":"/pdf/d3f077646fd700da8a67891c7dcb9f9eca9e9c44.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"XREF: Entity Linking for Chinese News Comments with Supplementary Article Reference","youtube":"XJCLlr3rdT0"},"forum":"1hLH6CKIjN","id":"42"},{"content":{"TLDR":"A novel unsupervised approach to mine domain-dependent variational attributes present in unstructured text and use it to identify entity pairs that are the same and pairs that are variations of each other.","abstract":"Presence of near identical, but distinct, entities called entity variations makes the task of data integration challenging. For example, in the domain of grocery products, variations share the same value for attributes such as brand, manufacturer and product line, but differ in other attributes, called variational attributes, such as package size and color. Identifying variations across data sources is an important task in itself and is crucial for identifying duplicates. However, this task is challenging as the variational attributes are often present as a part of unstructured text and are domain dependent. In this work, we propose our approach, Contrastive entity linkage, to identify both entity pairs that are the same and pairs that are variations of each other. We propose a novel unsupervised approach, VarSpot, to mine domain-dependent variational attributes present in unstructured text. The proposed approach reasons about both similarities and differences between entities and can easily scale to large sources containing millions of entities. We show the generality of our approach by performing experimental evaluation on three different domains. Our approach significantly outperforms state-of-the-art learning-based and rule-based entity linkage systems by up to 4% F1 score when identifying duplicates, and up to 41% when identifying entity variations.","authors":["Varun Embar","Bunyamin Sisman","Hao Wei","Xin Luna Dong","Christos Faloutsos","Lise Getoor"],"keywords":["entity resolution","entity linkage","variations","product linkage","product catalogs"],"pdf_url":"/pdf/328eed7cd685a1682ca22a7847de7bc08995baf9.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Contrastive Entity Linkage: Mining Variational Attributes from Large Catalogs for Entity Linkage","youtube":"qLv5G-lhLXs"},"forum":"fR44nF03Rb","id":"26"},{"content":{"TLDR":"We propose a scalable method to construct  the large-scale eventuality entailment graph with high precision. ","abstract":"Computational and cognitive studies suggest that the abstraction of eventualities (activities, states, and events) is crucial for humans to understand daily eventualities. In this paper, we propose a scalable approach to model the entailment relations between eventualities (\"eat an apple'' entails ''eat fruit''). As a result, we construct a large-scale eventuality entailment graph (EEG), which has 10 million eventuality nodes and 103 million entailment edges. Detailed experiments and analysis demonstrate the effectiveness of the proposed approach and quality of the resulting knowledge graph. Our datasets and code are available at https://github.com/HKUST-KnowComp/ASER-EEG.","authors":["Changlong Yu","Hongming Zhang","Yangqiu Song","Wilfred Ng","Lifeng Shang"],"keywords":["eventuality knowledge graph","entailment graph","commonsense reasoning"],"pdf_url":"/pdf/888876ed59decc02bf40752432660b526af5d17c.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Enriching Large-Scale Eventuality Knowledge Graph with Entailment Relations","youtube":"TIsZAGYeSyQ"},"forum":"-oXaOxy6up","id":"69"},{"content":{"TLDR":"Learning to predict relation entailment using both structured and textual information","abstract":"Relations among words and entities are important for semantic understanding of text, but previous work has largely not considered relations between relations, or meta-relations. In this paper, we specifically examine relation entailment, where the existence of one relation can entail the existence of another relation. Relation entailment allows us to construct relation hierarchies, enabling applications in representation learning, question answering, relation extraction, and summarization. To this end, we formally define the new task of predicting relation entailment and construct a dataset by expanding the existing Wikidata relation hierarchy without expensive human intervention. We propose several methods that incorporate both structured and textual information to represent relations for this task. Experiments and analysis demonstrate that this task is challenging, and we provide insights into task characteristics that may form a basis for future work. The dataset and code have been released at https://github.com/jzbjyb/RelEnt.","authors":["Zhengbao Jiang","Jun Araki","Donghan Yu","Ruohong Zhang","Wei Xu","Yiming Yang","Graham Neubig"],"keywords":["relation entailment","structured information","textual information"],"pdf_url":"/pdf/564a1529e2b80aba972e3434a5c2b9c855e155db.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Learning Relation Entailment with Structured and Textual Information","youtube":"pCmiDtfbbcA"},"forum":"ToTf_MX7Vn","id":"14"},{"content":{"TLDR":"Extending Box embeddings to model multiple relations","abstract":"Learning representations for hierarchical and multi-relational knowledge has emerged as an active area of research. Box Embeddings  [Vilnis et al., 2018, Li et al., 2019] represent concepts with hyperrectangles in $n$-dimensional space and are shown to be capable of modeling tree-like structures efficiently by training on a large subset of the transitive closure of the WordNet hypernym graph. In this work, we evaluate the capability of box embeddings to learn the transitive closure of a tree-like hierarchical relation graph with far fewer edges from the transitive closure. Box embeddings are not restricted to tree-like structures, however, and we demonstrate this by modeling the WordNet meronym graph, where nodes may have multiple parents. We further propose a method for modeling multiple relations jointly in a single embedding space using box embeddings. In all cases, our proposed method outperforms or is at par with all other embedding methods.","authors":["Dhruvesh Patel","Shib Sankar Dasgupta","Michael Boratko","Xiang Li","Luke Vilnis","Andrew McCallum"],"keywords":["embeddings","order embeddings","knowledge graph embedding","relational learning","hyperbolic entailment cones","knowledge graphs","transitive relations"],"pdf_url":"/pdf/b6de1849f0ca7a9ed23483c4d9420dccbd365f7b.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Representing Joint Hierarchies with Box Embeddings","youtube":"yqP8wjMocAs"},"forum":"J246NSqR_l","id":"91"},{"content":{"TLDR":"Predicting hierarchies of institutions by modeling set operations over tokens","abstract":"The hierarchical structure of research organizations plays a pivotal role in science of science research as well as in tools that track the research achievements and output. However, this structure is not consistently documented for all institutions in the world, motivating the need for automated construction methods. In this paper, we present a new task and model for predicting sub-institution/super-institution relationships based on their string names. The crux of our model is that it leverages learned, permutation invariant representations of various token subsets of institution name strings. Our model outperforms or matches non-set-based models and baselines. We also create a dataset for training and evaluating models for this task based on the publicly available relationships in the Global Research Identifier Database.","authors":["Derek Tam","Nicholas Monath","Ari Kobren","Andrew McCallum"],"keywords":["Hierarchies","Sets","Transformers","Institutions"],"pdf_url":"/pdf/aca5866d5727cd021e5a9b1e1bdcaa06555b5ad6.pdf","session":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"}],"title":"Predicting Institution Hierarchies with Set-based Models","youtube":"oYSgWGoYbJM"},"forum":"pJg1LahGc0","id":"49"}],"interactive_sessions":[{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"},{"additionalZoomIds":[73136861818],"calendarId":"hallway","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"17:30","link":"session_d2hallway.html","sessionId":"d2hallway","start":"16:55","title":"Day 2 Hallway Chats"}],"name":"Day 2","sessions":[{"calendarId":"invited","chatSession":"d2hallway","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"12:35","link":"speaker_lzettlemoyer.html","sessionId":"lzettlemoyer","start":"12:00","title":"Invited: Luke Zettlemoyer"},{"calendarId":"invited","chatSession":"d2hallway","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"13:10","link":"speaker_nnakashole.html","sessionId":"nnakashole","start":"12:35","title":"Invited: Ndapandula Nakashole"},{"calendarId":"invited","chatSession":"d2hallway","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"13:45","link":"speaker_estrubell.html","sessionId":"estrubell","start":"13:10","title":"Invited: Emma Strubell"},{"calendarId":"talks","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"14:45","link":"session_d2posters.html","sessionId":"d2talks","start":"13:45","title":"Day 2 Lightning Talks"},{"calendarId":"poster","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"15:45","link":"session_d2posters.html","sessionId":"d2posters","start":"14:45","title":"Day 2 Poster Session"},{"calendarId":"invited","chatSession":"d2hallway","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"16:20","link":"speaker_asu.html","sessionId":"asu","start":"15:45","title":"Invited: Andrew Su"},{"calendarId":"invited","chatSession":"d2hallway","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"16:55","link":"speaker_dyang.html","sessionId":"dyang","start":"16:20","title":"Invited: Diyi Yang"},{"additionalZoomIds":[73136861818],"calendarId":"hallway","date":"2020-06-23","day_name":"Day 2","day_uid":"d2","end":"17:30","link":"session_d2hallway.html","sessionId":"d2hallway","start":"16:55","title":"Day 2 Hallway Chats"}],"speakers":[{"UID":"lzettlemoyer","abstract":"Denoising auto-encoders can be pre-trained at a very large scale by noising and then reconstructing any input text. Existing methods, based on variations of masked languages models, have transformed the field and are now provide the de facto initialization to be tuned for nearly every task. In this talk, I will present our work on sequence-to-sequence pre-training that allows arbitrary noising, by simply learning to translate any corrupted text back to the original with standard Tranformer-based neural machine translation architectures. I will show the resulting mono-lingual (BART) and multi-lingual (mBART) models are highly effective for a wide range of discrimination and generation tasks, including question answer, summarization, and machine translation. A key contribution of our generalized noising is that we can replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance, as I will describe. Finally, I will highlight many of the ways BART is already being used by other researchers, and discuss opportunities to further push models that pre-train for generating and understanding text in many languages.","bio":"Luke Zettlemoyer is a Professor in the Paul G. Allen School of Computer Science & Engineering at the University of Washington, and a Research Scientist at Facebook. His research focuses on empirical methods for natural language semantics, and involves designing machine learning algorithms; introducing new tasks and datasets; and, most recently, studying how to best develop self-supervision signals for text. Honors include multiple paper awards, a PECASE award, and an Allen Distinguished Investigator Award. Luke received his PhD from MIT and was a postdoc at the University of Edinburgh.","institution":"University of Washington / Facebook","speaker":"Luke Zettlemoyer","thumbnail":"lzettlemoyer.jpg","title":"Denoising Sequence-to-Sequence Pre-training","url":"https://www.cs.washington.edu/people/faculty/lsz"},{"UID":"nnakashole","abstract":"How can text processing models be used to help self-directed students learn the skills they need to be effective data scientists, for example, the basics of Probability Theory? How can text processing models be used to automate mundane data wrangling tasks to help improve efficiency of Data Scientists? In this talk, I will discuss these questions, and our work on the NLP systems we are building to answer these questions.","bio":"Ndapa Nakashole is an Assistant Professor at the University of California, San Diego, where she teaches and carries out research on Statistical Natural Language Processing. Before that she was postdoctoral scholar at Carnegie Mellon University. She obtained her PhD from Saarland University and the Max Planck Institute for Informatics, Germany. She completed undergraduate studies in Computer Science at the University of Cape Town, South Africa.","institution":"University of California San Diego","speaker":"Ndapandula Nakashole","thumbnail":"nnakashole.jpg","title":"Text Processing for Learning and Automation of Data Science.","url":"http://nakashole.com/"},{"UID":"estrubell","abstract":"Large, pre-trained language models (LMs) like BERT produce high quality, general purpose representations of word(piece)s in context. Unfortunately, training and deploying these models comes at a high computational cost, limiting their development and use to a small set of institutions with access to substantial computational resources, while potentially accelerating climate change with their unprecedented energy requirements. In this talk I\u2019ll characterize the inefficiencies of LM training and decoding, survey recent techniques for scaling down large pre-trained language models, and identify potential exciting research directions with the goal of enabling a broader array of researchers and practitioners to benefit from these powerful models, while remaining mindful of the environmental impact of our work.","bio":"Emma Strubell is a Visiting Researcher at Facebook AI Research, and Assistant Professor in the Language Technologies Institute at Carnegie Mellon University. Her research aims to provide fast and robust natural language processing to the diversity of academic and industrial investigators eager to pull insight and decision support from massive text data in many domains. Toward this end she works at the intersection of natural language understanding, machine learning, and deep learning methods cognizant of modern tensor processing hardware. Her research has been recognized with best paper awards at ACL 2015 and EMNLP 2018.","institution":"Facebook / CMU","speaker":"Emma Strubell","thumbnail":"estrubell.jpg","title":"Learning to live with BERT","url":"https://people.cs.umass.edu/~strubell/"},{"UID":"asu","abstract":"The biomedical research community is incredibly productive, producing over one million new publications per year. However, the knowledge contained in those publications usually remains in unstructured free text, or is fragmented across unconnected data silos. Here, I will describe recent efforts to integrate biomedical knowledge into large, heterogeneous knowledge graphs, and to mine those knowledge graphs to identify novel testable hypotheses","bio":"Andrew is a Professor at the Scripps Research Institute in the Department of Integrative Structural and Computational Biology (ISCB). His research focuses on building and applying bioinformatics infrastructure for biomedical discovery. His research has a particular emphasis on leveraging crowdsourcing for genetics and genomics. Representative projects include the Gene Wiki, BioGPS, MyGene.Info, and Mark2Cure, each of which engages the crowd to help organize biomedical knowledge. These resources are collectively used millions of times every month by members of the research community, by students, and by the general public.","institution":"Scripps Institute","speaker":"Andrew Su","thumbnail":"asu.jpg","title":"Building and mining a heterogenous biomedical knowledge graph","url":"https://www.scripps.edu/faculty/su/"},{"UID":"dyang","abstract":"Over the last few decades, natural language processing (NLP) has had increasing success and produced industrial applications like search, and personal assistants. Despite being sufficient to enable these applications, current NLP systems largely ignore the social part of language, e.g., who says it, in what context, for what goals. My research combines NLP, linguistics and social science to study how people use language in different social settings for their social goals, with the implications of developing systems to facilitate human-human and human-machine communication. In this talk, I will explain my research from two specific studies. The first part studies what makes language persuasive by introducing a semi-supervised neural network to recognize persuasion strategies in loan requests on crowdfunding platforms, and further designed neural encoder-decoder systems to automatically transform inappropriately subjective framing into a neutral point of view. The second focuses on modeling how people seek and offer support via language in online cancer support communities and building interventions to support patient communication. Through these examples, I show how we can accurately and efficiently build better language technologies for social contexts.","bio":"Diyi Yang is an assistant professor in the School of Interactive Computing at Georgia Tech, also affiliated with the Machine Learning Center (ML@GT) at Georgia Tech. She is interested in natural language processing (e.g., language generation, semantics, discourse), and computational social science. Diyi received her PhD from the Language Technologies Institute at Carnegie Mellon University, and her bachelor's degree from Shanghai Jiao Tong University, China. Her work has been published at leading NLP/HCI conferences, and also resulted in multiple award-winning papers from EMNLP 2015 and SIGCHI 2019. She has served as an Area Chair for ACL, EMNLP, NAACL, CIKM, and CSCW conferences.","institution":"Georgia Institute of Technology","speaker":"Diyi Yang","thumbnail":"dyang.jpg","title":"Language Understanding in Social Context","url":"https://www.cc.gatech.edu/~dyang888/experience.html"}],"webinar":{"UID":95610670073,"link":"https://uci.zoom.us/j/95610670073","meeting_password":"none"},"youtube":"KTQPWoQ7Ol8"},{"UID":"d3","date":"2020-06-24","highlighted":[{"content":{"TLDR":"A new paradigm for deep contextualized knowledge graph embeddings","abstract":"We introduce Dolores, a new knowledge graph embeddings, that effectively capture contextual cues and dependencies among entities and relations. First, we note that short paths on knowledge graphs comprising of chains of entities and relations can encode valuable information regarding their contextual usage. We operationalize this notion by representing knowledge graphs not as a collection of triples but as a collection of entity-relation chains, and learn embeddings using deep neural models that capture such contextual usage. Based on BiLSTMs, our model learns deep representations from constructed entity-relation chains. We show that these representations can be easily incorporated into existing models to significantly advance the performance on several knowledge graph tasks like link prediction, triple classification, and multi-hop knowledge base completion.","authors":["Haoyu Wang","Vivek Kulkarni","William Yang Wang"],"keywords":["Knowledge Graph","Contextualized Embeddings"],"pdf_url":"/pdf/5def4d7bb7910beb50c324f1d628ad945b27da0f.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"Dolores: Deep Contextualized Knowledge Graph Embeddings","youtube":"z4eB1cmcGm4"},"forum":"ajrveGQBl0","id":"10"},{"content":{"TLDR":"We investigate the combined use of ontologies and embeddings in KG refinement task.","abstract":"Knowledge Graphs (KGs) extracted from text sources are often noisy and lead to poor performance in downstream application tasks such as KG-based question answering. While much of the recent activity is focused on addressing the sparsity of KGs by using embeddings for inferring new facts, the issue of cleaning up of noise in KGs through KG refinement task is not as actively studied. Most successful techniques for KG refinement make use of inference rules and reasoning over ontologies. Barring a few exceptions, embeddings do not make use of ontological information, and their performance in KG refinement task is not well understood. In this paper, we present a KG refinement framework called IterefinE which iteratively combines the two techniques \u2013 one which uses ontological information and inferences rules, viz.,PSL-KGI, and the KG embeddings such as ComplEx and ConvE which do not. As a result, IterefinE is able to exploit not only the ontological information to improve the quality of predictions, but also the power of KG embeddings which (implicitly) perform longer chains of reasoning. The IterefinE framework, operates in a co-training mode and results in explicit type-supervised embeddings of the refined KG from PSL-KGI which we call as TypeE-X. Our experiments over a range of KG benchmarks show that the embeddings that we produce are able to reject noisy facts from KG and at the same time infer higher quality new facts resulting in upto 9% improvement of overall weighted F1 score.","authors":["Siddhant Arora","Srikanta Bedathur","Maya Ramanath","Deepak Sharma"],"keywords":["Knowledge graph refinement","embeddings","inference"],"pdf_url":"/pdf/858984f9aa759b64181c7f0d4ae717fc9418b018.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"IterefinE: Iterative KG Refinement Embeddings using Symbolic Knowledge","youtube":"Sd1ZPMY2XHc"},"forum":"fCQvGMT57w","id":"18"},{"content":{"TLDR":"We advance CSK towards a more expressive stage of multifaceted knowledge, and use joint reasoning over all statements to ensure coherence and combat sparsity.","abstract":"Commonsense knowledge (CSK) supports a variety of AI applications, from visual understanding to chatbots. Prior works on acquiring CSK, such as ConceptNet, have compiled statements that associate concepts with properties that hold for most or some of their instances. Each concept and statement is treated in isolation from others, and the only quantitative measure (or ranking) is a confidence score that the statement is valid. This paper aims to overcome these limitations by introducing a multi-faceted model of CSK statements and methods for joint reasoning over sets of inter-related statements. Our model captures four different dimensions of CSK statements: plausibility, typicality, remarkability and salience, with scoring and ranking along each dimension. For example, hyenas drinking water is typical but not salient, whereas hyenas eating carcasses is salient. For reasoning and ranking, we develop a method with soft constraints, to couple the inference over concepts that are related in a taxonomic hierarchy. The reasoning is cast into an integer linear programming (ILP), and we leverage the theory of reduction costs of a relaxed LP to compute informative rankings. Our evaluation shows that we can consolidate existing CSK collections into much cleaner and more expressive knowledge.","authors":["Yohan Chalier","Simon Razniewski","Gerhard Weikum"],"keywords":["Commonsense knowledgebase construction"],"pdf_url":"/pdf/f56e2fd777ea63f87a20c7a84e728c45b2d659de.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"Joint Reasoning for Multi-Faceted Commonsense Knowledge","youtube":"IMHlhIw4hCA"},"forum":"QnPV72SZVt","id":"2"},{"content":{"TLDR":"A data-centric domain adaptation framework for  KG question answering for unseen domains","abstract":"Knowledge Graph Simple Question Answering (KGSQA), in its standard form, does not take into account that human-curated question answering training data only cover a small subset of the relations that exist in a Knowledge Graph (KG), or even worse, that new domains covering unseen and rather different to existing domains relations are added to the KG. In this work, we study KGQA for first-order questions in a previously unstudied setting where new, unseen, domains are added during test time. In this setting, question-answer pairs of the new domain do not appear during training, thus making the task more challenging. We propose a data-centric domain adaptation framework that consists of a KGQA system that is applicable to new domains, and a sequence to sequence question generation method that automatically generates question-answer pairs for the new domain. Since the effectiveness of question generation for KGQA can be restricted by the limited lexical variety of the generated questions, we use distant supervision to extract a set of keywords that express each relation of the unseen domain and incorporate those in the question generation method. Experimental results demonstrate that our framework significantly improves over zero-shot baselines and is robust across domains.","authors":["Georgios Sidiropoulos","Nikos Voskarides","Evangelos Kanoulas"],"keywords":["Question Answering","Knowledge Graph","Domain Adaptation"],"pdf_url":"/pdf/80ca76350ab431fb1875b73448f00dca13a0fdb1.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"Knowledge Graph Simple Question Answering for Unseen Domains","youtube":"HFXRLaYP0OU"},"forum":"Ie2Y94Ty8K","id":"35"},{"content":{"TLDR":"We present an outcome explanation engine for Factorization based KBC","abstract":"State-of-the-art models for Knowledge Base Completion (KBC) are based on tensor factorization (TF), e.g, DistMult, ComplEx. While they produce good results, they cannot expose any rationale behind their predictions, potentially reducing the trust of a user in the model. Previous works have explored creating an inherently explainable model, e.g. Neural Theorem Proving (NTP), DeepPath, MINERVA, but explainability comes at the cost of performance. Others have tried to create an auxiliary explainable model having high fidelity with the underlying TF model, but unfortunately, they do not scale on large KBs such as FB15k and YAGO.\u00a0In this work, we propose OxKBC -- an Outcome eXplanation engine for KBC, which provides a post-hoc explanation for every triple inferred by an (uninterpretable) factorization based model. It first augments the underlying Knowledge Graph by introducing weighted edges between entities based on their similarity given by the underlying model. In the augmented graph, it defines a notion of human-understandable explanation paths along with a language to generate them. Depending on the edges, the paths are aggregated into second-order templates for further selection. The best template with its grounding is then selected by a neural selection module that is trained with minimal supervision by a novel loss function. Experiments over Mechanical Turk demonstrate that users find our explanations more trustworthy compared to rule mining.","authors":["Yatin Nandwani","Ankesh Gupta","Aman Agrawal","Mayank Singh Chauhan","Parag Singla","Mausam"],"keywords":["xai","kbc","templates","outcome explanation","templates"],"pdf_url":"/pdf/45344fe9807a05403de6f6b8cce593d126b9e3b4.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"OxKBC: Outcome Explanation for Factorization Based Knowledge Base Completion","youtube":"n7PXsp1WZBI"},"forum":"nqYhFwaUj","id":"55"},{"content":{"TLDR":"Most knowledge bases so far only contain positive information. We argue for the importance of negative information, and present two methods to mine it.","abstract":"Knowledge bases (KBs), pragmatic collections of knowledge about notable entities, are an important asset in applications such as search, question answering and dialogue. Rooted in a long tradition in knowledge representation, all popular KBs only store positive information, but abstain from taking any stance towards statements not contained in them.\n\nIn this paper, we make the case for explicitly stating interesting statements which are not true. Negative statements would be important to overcome current limitations of question answering, yet due to their potential abundance, any effort towards compiling them needs a tight coupling with ranking. We introduce two approaches towards automatically compiling negative statements. (i) In peer-based statistical inferences, we compare entities with highly related entities in order to derive potential negative statements, which we then rank using supervised and unsupervised features. (ii) In pattern-based query log extraction, we use a pattern-based approach for harvesting search engine query logs. Experimental results show that both approaches hold promising and complementary potential. Along with this paper, we publish the first datasets on interesting negative information, containing over 1.4M statements for 130K popular Wikidata entities.","authors":["Hiba Arnaout","Simon Razniewski","Gerhard Weikum"],"keywords":["information retrieval","knowledge bases","ranking","negation"],"pdf_url":"/pdf/3b6dc13840b1eb2ac65949374068d574bd2bda26.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"Enriching Knowledge Bases with Interesting Negative Statements","youtube":"Q-C2MbzGXjc"},"forum":"pSLmyZKaS","id":"4"},{"content":{"TLDR":"Retrieving medical information from oncology literature using NLP. ","abstract":"The vast and rapidly expanding volume of biomedical literature makes it difficult for domain experts to keep up with the evidence. In this work, we specifically consider the exponentially growing subarea of genetics in cancer. The need to synthesize and centralize this evidence for dissemination has motivated a team of physicians (with whom this work is a collaboration) to manually construct and maintain a knowledge base that distills key results reported in the literature. This is a laborious process that entails reading through full-text articles to understand the study design, assess study quality, and extract the reported cancer risk estimates associated with particular hereditary cancer genes (i.e., \\emph{penetrance}). In this work, we propose models to automatically surface key elements from full-text cancer genetics articles, with the ultimate aim of expediting the manual workflow currently in place.\n\nWe propose two challenging tasks that are critical for characterizing the findings reported cancer genetics studies: (i) Extracting snippets of text that describe \\emph{ascertainment mechanisms}, which in turn inform whether the population studied may introduce bias owing to deviations from the target population; (ii) Extracting reported risk estimates (e.g., odds or hazard ratios) associated with specific germline mutations. The latter task may be viewed as a joint entity tagging and relation extraction problem. To train models for these tasks, we induce distant supervision over tokens and snippets in full-text articles using the manually constructed knowledge base. We propose and evaluate several model variants, including a transformer-based joint entity and relation extraction model to extract \\texttt{<germline mutation, risk-estimate>} pairs. We observe strong empirical performance, highlighting the practical potential for such models to aid KB construction in this space. We ablate components of our model, observing, e.g., that a joint model for \\texttt{<germline mutation, risk-estimate>} fares substantially better than a pipelined approach. ","authors":["Somin Wadhwa","Kanhua Yin","Kevin S. Hughes","Byron Wallace"],"keywords":["Cancer genetics","biomedical nlp","information extraction","clinical informatics","knowledge base construction"],"pdf_url":"/pdf/948bfb23353a017c1a16b1f435a871e64bc2af38.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"Semi-Automating Knowledge Base Construction for Cancer Genetics","youtube":"TgJINTJLYJA"},"forum":"EQrvONEwh","id":"50"},{"content":{"TLDR":"We publish a new evaluation benchmark for knowledge graph completion methods where ranking is replaced with actual classification, and show one way to improve knowledge graph embedding models in this new setting.","abstract":"Knowledge base completion (KBC) methods aim at inferring missing facts from the information present in a knowledge base (KB). Such a method thus needs to estimate the likelihood of candidate facts and ultimately to distinguish between true facts and false ones to avoid compromising the KB with untrue information. In the prevailing evaluation paradigm, however, models do not actually decide whether a new fact should be accepted or not but are solely judged on the position of true facts in a likelihood ranking with other candidates. We argue that consideration of binary predictions is essential to reflect the actual KBC quality, and propose a novel evaluation paradigm, designed to provide more transparent model selection criteria for a realistic scenario. We construct the data set FB14k-QAQ with an alternative evaluation data structure: instead of single facts, we use KB queries, i.e., facts where one entity is replaced with a variable, and construct corresponding sets of entities that are correct answers. We randomly remove some of these correct answers from the data set, simulating the realistic scenario of real-world entities missing from a KB. This way, we can explicitly measure a model\u2019s ability to handle queries that have more correct answers in the real world than in the KB, including the special case of queries without any valid answer. The latter especially contrasts the ranking setting. We evaluate a number of state-of-the-art KB embeddings models on our new benchmark. The differences in relative performance between ranking-based and classification-based evaluation that we observe in our experiments confirm our hypothesis that good performance on the ranking task does not necessarily translate to good performance on the actual completion task. Our results motivate future work on KB embedding models with better prediction separability and, as a first step in that direction, we propose a simple variant of TransE that encourages thresholding and achieves a significant improvement in classification F 1 score relative to the original TransE.","authors":["Marina Speranskaya","Martin Schmitt","Benjamin Roth"],"keywords":["knowledge base completion","knowledge graph embedding","classification","ranking"],"pdf_url":"/pdf/a11e6a97c8d61b1e10d72dffde74e70d67bf136e.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"Ranking vs. Classifying: Measuring Knowledge Base Completion Quality","youtube":"5P5oTKkN9XM"},"forum":"3pcecaCEK-","id":"81"},{"content":{"TLDR":"We use semantic relations associated with mentions to improve fine-grained entity typing.","abstract":"Fine-grained entity typing results can serve as important information for entities while constructing knowledge bases. It is a challenging task due to the use of large tag sets and the requirement of understanding the context.\nWe find that, in some cases, existing neural fine-grained entity typing models may ignore the semantic information in the context that is important for typing. \nTo address this problem, we propose to exploit semantic relations extracted from the sentence to improve the use of context. The used semantic relations are mainly those that are between the mention and the other words or phrases in the sentence. \nWe investigate the use of two types of semantic relations: hypernym relation, and verb-argument relation. Our approach combine the predictions made based on different semantic relations and the predictions of a base neural model to produce the final results. We conduct experiments on two commonly used datasets: FIGER (GOLD) and BBN. Our approach achieves at least 2\\% absolute strict accuracy improvement on both datasets compared with a strong BERT based model.","authors":["Hongliang Dai","Yangqiu Song","Xin Li"],"keywords":["Fine-grained Entity Typing","Hypernym Extraction","Semantic Role Labeling"],"pdf_url":"/pdf/b91e260e0c6b51dbad1183607c61b5490ccb0b49.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"Exploiting Semantic Relations for Fine-grained Entity Typing","youtube":"1jZrb0T0qR8"},"forum":"BSUYfTada3","id":"6"},{"content":{"TLDR":"","abstract":"Knowledge graph (KG) representation learning techniques that learn continuous embeddings of entities and relations in the KG have become popular in many AI applications. With a large KG, the embeddings consume a large amount of storage and memory. This is problematic and prohibits the deployment of these techniques in many real world settings. Thus, we propose an approach that compresses the KG embedding layer by representing each entity in the KG as a vector of discrete codes and then composes the embeddings from these codes. The approach can be trained end-to-end with simple modifications to any existing KG embedding technique. We evaluate the approach on various standard KG embedding evaluations and show that it achieves 50-1000x compression of embeddings with a minor loss in performance. The compressed embeddings also retain the ability to perform various reasoning tasks such as KG inference.","authors":["Mrinmaya Sachan"],"keywords":[],"pdf_url":"","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"Knowledge Graph Embedding Compression","youtube":"kBmU2pPMHCA"},"forum":"SOEJGCE76x","id":"76"},{"content":{"TLDR":"learning tractable models for imprecise probabilities ","abstract":"Probabilistic representations, such as Bayesian and Markov networks, are fundamental to much of statistical machine learning. Thus, learning probabilistic representations directly from data is a deep challenge, the main computational bottleneck being inference that is intractable. Tractable learning is a powerful new paradigm that attempts to learn distributions that support efficient probabilistic querying. By leveraging local structure, representations such as sum-product networks (SPNs) can capture high tree-width models with many hidden layers, essentially a deep architecture, while still admitting a range of probabilistic queries to be computable in time polynomial in the network size.  While the progress is impressive, numerous data sources are incomplete, and in the presence of missing data, structure learning methods nonetheless revert to  single distributions without  characterizing the loss in confidence. In recent work, credal sum-product networks, an imprecise extension of sum-product networks, were proposed to capture this robustness angle. In this work, we are interested in how such representations can be learnt and thus study how the computational machinery underlying tractable learning and inference can be generalized for imprecise probabilities. \n","authors":["Amelie Levray","Vaishak Belle"],"keywords":["credal networks","imprecise probabilities","tractable learning"],"pdf_url":"/pdf/961305c5ab559cf0e3017db830b746377472625b.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"Learning Credal Sum-Product Networks","youtube":"IJtLSzsQYaQ"},"forum":"3-Tc21z1Ub","id":"12"},{"content":{"TLDR":"We propose TransINT, a novel and interpretable KG embedding method that isomorphically preserves the implication ordering among relations in the embedding space in an explainable, robust, and geometrically coherent way.","abstract":"Knowledge Graphs (KG), composed of entities and relations, provide a structured representation of knowledge. For easy access to statistical approaches on relational data, multiple methods to embed a KG into f(KG) \u2208 R^d have been introduced. We propose TransINT, a novel and interpretable KG embedding method that isomorphically preserves the implication ordering among relations in the embedding space. Given implication rules, TransINT maps set of entities (tied by a relation) to continuous sets of vectors that are inclusion-ordered isomorphically to relation implications. With a novel parameter sharing scheme, TransINT enables automatic training on missing but implied facts without rule grounding. On a benchmark dataset, we outperform the best existing state-of-the-art rule integration embedding methods with significant margins in link Prediction and triple Classification. The angles between the continuous sets embedded by TransINT provide an interpretable way to mine semantic relatedness and implication rules among relations.","authors":["So Yeon Min","Preethi Raghavan","Peter Szolovits"],"keywords":["Knowledge Graph Embedding","Isomorphism","Rules","Common Sense","Implication Rules","Knowledge Graph","Isomorphic Embedding","Semantics Mining","Rule Mining","TransH"],"pdf_url":"/pdf/e9d03f36d42f9d263006c982f5e0a082e17e5782.pdf","session":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"}],"title":"TransINT: Embedding Implication Rules in Knowledge Graphs with Isomorphic Intersections of Linear Subspaces","youtube":"HS8TJGpLeJs"},"forum":"shkmWLRBXH","id":"87"}],"interactive_sessions":[{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"},{"additionalZoomIds":[73967891090],"calendarId":"hallway","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"13:35","link":"session_d3hallway.html","sessionId":"d3hallway","start":"12:35","title":"Day 3 Hallway Chats"}],"name":"Day 3","sessions":[{"calendarId":"invited","chatSession":"d3hallway","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"08:50","link":"speaker_jberant.html","sessionId":"jberant","start":"08:15","title":"Invited: Jonathan Berant"},{"calendarId":"invited","chatSession":"d3hallway","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"09:25","link":"speaker_bvandurme.html","sessionId":"bvandurme","start":"08:50","title":"Invited: Benjamin Van Durme"},{"calendarId":"talks","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"10:25","link":"session_d3posters.html","sessionId":"d3talks","start":"09:25","title":"Day 3 Lightning Talks"},{"calendarId":"poster","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"11:25","link":"session_d3posters.html","sessionId":"d3posters","start":"10:25","title":"Day 3 Poster Session"},{"calendarId":"invited","chatSession":"d3hallway","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"12:00","link":"speaker_trekatsinas.html","sessionId":"trekatsinas","start":"11:25","title":"Invited: Theodoros Rekatsinas"},{"calendarId":"invited","chatSession":"d3hallway","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"12:35","link":"speaker_hji.html","sessionId":"hji","start":"12:00","title":"Invited: Heng Ji"},{"additionalZoomIds":[73967891090],"calendarId":"hallway","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"13:35","link":"session_d3hallway.html","sessionId":"d3hallway","start":"12:35","title":"Day 3 Hallway Chats"},{"calendarId":"---","date":"2020-06-24","day_name":"Day 3","day_uid":"d3","end":"13:45","link":"","sessionId":"closing","start":"13:35","title":"Closing Remarks"}],"speakers":[{"UID":"jberant","abstract":"Models for question answering (QA) have been dominated recently by fully differentiable neural networks, based on large pre-trained language models. However, when answering a question requires multiple reasoning steps, symbolic approaches offer a natural and often more interpretable alternative. In this talk, I will describe recent work that focuses on the pros and cons of symbolic and distributed approaches for question answering. First, I will describe QDMR, a symbolic meaning representation for questions, inspired by semantic parsing, that can be annotated at scale by non-experts. QDMR was used to annotate BREAK: a benchmark for question understanding that contains 83K questions with their meaning representation from 10 existing datasets across three modalities. Then, I will show how a symbolic representation, such as QDMR, can be used to (a) improve accuracy on open-domain QA benchmarks that require multiple retrieval steps (b) improve the faithfulness of compositional neural networks for answering complex questions. I will then move to two cases where end-to-end differentiable models provide advantages over symbolic approaches. Specifically, one can automatically generate data at scale for both numerical and logical reasoning, and easily endow pre-trained language models with those missing capabilities for answering complex questions.","bio":"Jonathan Berant is a senior lecturer (assistant professor) at the School of Computer Science at Tel Aviv University and a research scientist at The Allen Institute for AI. Jonathan earned a Ph.D. in Computer Science at Tel-Aviv University, under the supervision of Prof. Ido Dagan. Jonathan was a post-doctoral fellow at Stanford University, working with Prof. Christopher Manning and Prof. Percy Liang, and subsequently a post-doctoral fellow at Google Research, Mountain View. Jonathan Received several awards and fellowships including The Rothschild fellowship, The ACL 2011 best student paper award, EMNLP 2014 best paper award, and NAACL 2019 best resource paper award, as well as several honorable mentions. Jonathan is currently an ERC grantee.","institution":"Tel Aviv University / Allen Institute for AI","speaker":"Jonathan Berant","thumbnail":"jberant.png","title":"Symbolic and distributed representations for question answering","url":"http://www.cs.tau.ac.il/~joberant/"},{"UID":"bvandurme","abstract":"The Information Extraction and Computational Semantics communities are largely dominated by resources and models that strive for extracting what an observation categorically supports as a true prediction. E.g., \"this image is of a CAT\", or \"that token sequence refers to an ORGANIZATION\", or \"this sentence ENTAILS that other sentence\". As humans we recognize that observations can be ambiguous as to what predictions they evince, but we seem to forget that when building datasets, and then blindly aim to reproduce those annotations when building models. I will discuss a series of projects that explore annotating and modeling subjective likelihood assessments, with a focus on tasks such as semantic parsing and textual entailment. For example, the sentence \"Someone is walking a dog in a park\" may be interpreted as strong evidence for, \"The dog is alive\", weak evidence for, \"The sun is shining\", and cast doubt on (but not strictly \"contradict\"), \"The park is on fire\". While our work has concentrated on text, the point applies broadly: how often have you done an image captcha and had hesitation on whether that one picture contained a \"bridge\"? Let's agree that sometimes the right answer is \"maybe\".","bio":"Benjamin Van Durme is an Associate Professor of Computer Science at Johns Hopkins University and a Principal Research Scientist at Microsoft Semantic Machines. His research focusses on resources and models for natural language understanding. His observations on the problem of reporting bias in common sense knowledge acquisition was recognized with a best paper award at AKBC 2013.","institution":"John Hopkins University","speaker":"Benjamin Van Durme","thumbnail":"bvandurme.jpg","title":"Embracing uncertainty as the target of prediction","url":"https://www.cs.jhu.edu/~vandurme/"},{"UID":"trekatsinas","abstract":"Data quality management is a bottleneck in modern analytics as high-effort tasks such as data validation and cleaning are essential to obtain accurate results. This talk describes how to use machine learning to automate routine data quality management tasks. I will first introduce Probabilistic Unclean Databases (PUDs), a formal probabilistic framework to describe the quality of structured data and demonstrate how data validation and cleaning correspond to learning and inference problems over structured data distributions. I will then show how the PUDs framework forms the basis of the HoloClean framework, a state-of-the-art ML-based solution to automate data quality management for structured data. Finally, I will close with a discussion on lessons learned from HoloClean with particular emphasis on when accurate, automated data cleaning is feasible.","bio":"Theodoros (Theo) Rekatsinas is an Assistant Professor in the Department of Computer Sciences at the University of Wisconsin-Madison. He is a member of the Database Group. He is also a co-founder of inductiv, a startup focusing on automating data quality ops for analytical pipelines. Theo earned his Ph.D. in Computer Science from the University of Maryland and was a Moore Data Postdoctoral Fellow at Stanford University. His research interests are in data management, with a focus on data integration, data cleaning, and uncertain data. Theo's work has been recognized with an Amazon Research Award in 2017, a Best Paper Award at SDM 2015, and the Larry S. Davis Doctoral Dissertation award in 2015.","institution":"University of Wisconsin-Madison","speaker":"Theodoros Rekatsinas","thumbnail":"trekatsinas.jpg","title":"Automating Data Quality Management","url":"http://pages.cs.wisc.edu/~thodrek/"},{"UID":"hji","abstract":"Understanding events and communicating about them are fundamental human activities. However, it's much more difficulty to populate event-related knowledge compared to entity-related knowledge. For example, most people in the United States will be able to answer the question \"Who is President Barack Obama\u2019s wife?\", but very few people can give a complete answer to \"Who died in September 11 attacks?\". We propose a new research direction on event-centric knowledge base construction from multimedia multilingual sources. Our minds represent events at various levels of granularity and abstraction, which allows us to quickly access and reason about old and new scenarios. Progress in natural language understanding and computer vision has helped automate some parts of event understanding but the current, first-generation, automated event understanding is overly simplistic since it is local, sequential and flat. Real events are hierarchical and probabilistic. Understanding them requires knowledge in the form of a repository of abstracted event schemas (complex event templates), understanding the progress of time, using background knowledge, and performing global inference. Our approach to second-generation event understanding builds on an incidental supervision approach to inducing an event schema repository that is probabilistic, hierarchically organized and semantically coherent. Low level primitive components of event schemas are abundant, and can be part of multiple, sparsely occurring, higher-level schemas. Consequently, we combine bottom-up data driven approaches across multiple modalities with top-down consolidation of information extracted from a smaller number of encyclopedic resources. This facilitates inducing higher-level event and time representations analysts can interact with, and allow them to guide further reasoning and extract events by constructing a novel structured cross-media common semantic space. When complex events unfold in an emergent and dynamic manner, the multimedia multilingual digital data from traditional news media and social media often convey conflicting information. To understand the many facets of such complex, dynamic situations, we have also developed cross-media cross-document event coreference resolution and event-event relation tracking methods for event-centric knowledge population.","bio":"Heng Ji is a professor at Computer Science Department of University of Illinois at Urbana-Champaign. She received her B.A. and M. A. in Computational Linguistics from Tsinghua University, and her M.S. and Ph.D. in Computer Science from New York University. Her research interests focus on Natural Language Processing, especially on Information Extraction and Knowledge Base Population. She is selected as \"Young Scientist\" and a member of the Global Future Council on the Future of Computing by the World Economic Forum in 2016 and 2017. The awards she received include \"AI's 10 to Watch\" Award by IEEE Intelligent Systems in 2013 and NSF CAREER award in 2009. She has coordinated the NIST TAC Knowledge Base Population task since 2010. She has served as the Program Committee Co-Chair of many conferences including NAACL-HLT2018.","institution":"University of Illinois Urbana-Champaign","speaker":"Heng Ji","thumbnail":"hji.png","title":"Event-centric Knowledge Base Construction","url":"http://blender.cs.illinois.edu/hengji.html"}],"webinar":{"UID":95610670073,"link":"https://uci.zoom.us/j/95610670073","meeting_password":"none"},"youtube":"EIoy9xSabYs"},{"UID":"d4","date":"2020-06-25","highlighted":[],"interactive_sessions":[],"name":"Day 4","sessions":[{"calendarId":"---","date":"2020-06-25","day_name":"Day 4","day_uid":"d4","end":"13:00","link":"workshops.html","sessionId":"workshops","start":"08:00","title":"Workshops"}],"speakers":[]}]
